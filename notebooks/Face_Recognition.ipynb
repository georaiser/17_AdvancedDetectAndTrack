{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z1yWGDwzAf9P"
      },
      "outputs": [],
      "source": [
        "#!pip install ultralytics face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQz2DfavAYN_",
        "outputId": "daf5383a-ae10-484c-b8cd-d7cbb6b5c760"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hellraiser/miniforge3/envs/torch_env/lib/python3.11/site-packages/face_recognition_models/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_filename\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not find a face in the reference image.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x7fb4591825b0>, array([[[202, 202, 195],\n        [200, 200, 193],\n        [200, 200, 193],\n        ...,\n        [230, 236, 231],\n        [230, 236, 231],\n        [229, 235, 230]],\n\n       [[201, 201, 194],\n        [201, 201, 194],\n        [201, 201, 194],\n        ...,\n        [230, 236, 231],\n        [229, 235, 230],\n        [230, 236, 231]],\n\n       [[201, 201, 194],\n        [202, 202, 195],\n        [202, 202, 195],\n        ...,\n        [230, 236, 231],\n        [230, 236, 231],\n        [230, 236, 231]],\n\n       ...,\n\n       [[ 23,  23,  36],\n        [ 22,  24,  38],\n        [ 23,  25,  39],\n        ...,\n        [ 14,  19,  32],\n        [ 13,  18,  31],\n        [ 14,  16,  29]],\n\n       [[ 22,  21,  37],\n        [ 22,  24,  38],\n        [ 22,  24,  38],\n        ...,\n        [ 16,  18,  32],\n        [ 14,  16,  29],\n        [ 12,  12,  26]],\n\n       [[ 20,  20,  34],\n        [ 20,  22,  35],\n        [ 20,  22,  35],\n        ...,\n        [ 15,  17,  31],\n        [ 11,  14,  27],\n        [ 11,  11,  25]]], shape=(410, 436, 3), dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x7fb45916c370>, 1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    100\u001b[39m     cv2.destroyAllWindows()\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Reconocimiento facial usando el modelo HOG en la región detectada por YOLO\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Use the HOG model for face detection within the person crop\u001b[39;00m\n\u001b[32m     62\u001b[39m face_locations = face_recognition.face_locations(person_crop, model=\u001b[33m\"\u001b[39m\u001b[33mhog\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m face_encodings = \u001b[43mface_recognition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperson_crop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m name = \u001b[33m\"\u001b[39m\u001b[33mDesconocido\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Compare detected faces within the crop to known faces\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_env/lib/python3.11/site-packages/face_recognition/api.py:214\u001b[39m, in \u001b[36mface_encodings\u001b[39m\u001b[34m(face_image, known_face_locations, num_jitters, model)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_landmarks\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_env/lib/python3.11/site-packages/face_recognition/api.py:214\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [np.array(\u001b[43mface_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
            "\u001b[31mTypeError\u001b[39m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x7fb4591825b0>, array([[[202, 202, 195],\n        [200, 200, 193],\n        [200, 200, 193],\n        ...,\n        [230, 236, 231],\n        [230, 236, 231],\n        [229, 235, 230]],\n\n       [[201, 201, 194],\n        [201, 201, 194],\n        [201, 201, 194],\n        ...,\n        [230, 236, 231],\n        [229, 235, 230],\n        [230, 236, 231]],\n\n       [[201, 201, 194],\n        [202, 202, 195],\n        [202, 202, 195],\n        ...,\n        [230, 236, 231],\n        [230, 236, 231],\n        [230, 236, 231]],\n\n       ...,\n\n       [[ 23,  23,  36],\n        [ 22,  24,  38],\n        [ 23,  25,  39],\n        ...,\n        [ 14,  19,  32],\n        [ 13,  18,  31],\n        [ 14,  16,  29]],\n\n       [[ 22,  21,  37],\n        [ 22,  24,  38],\n        [ 22,  24,  38],\n        ...,\n        [ 16,  18,  32],\n        [ 14,  16,  29],\n        [ 12,  12,  26]],\n\n       [[ 20,  20,  34],\n        [ 20,  22,  35],\n        [ 20,  22,  35],\n        ...,\n        [ 15,  17,  31],\n        [ 11,  14,  27],\n        [ 11,  11,  25]]], shape=(410, 436, 3), dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x7fb45916c370>, 1"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import face_recognition\n",
        "\n",
        "def main():\n",
        "    # Inicializar modelos\n",
        "    yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "    # Cargar personas conocidas\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "\n",
        "    # Agregar persona de referencia\n",
        "    # Check if the reference image file exists before attempting to load it\n",
        "    try:\n",
        "        reference_image = face_recognition.load_image_file(\"JRE.jpg\")\n",
        "        # Use the HOG model for face detection which runs on CPU\n",
        "        reference_encoding = face_recognition.face_encodings(reference_image, model=\"hog\")\n",
        "        if reference_encoding: # Ensure encoding was successful\n",
        "            known_face_encodings.append(reference_encoding[0]) # Append the first encoding found\n",
        "            known_face_names.append(\"Persona Objetivo\")\n",
        "        else:\n",
        "            print(\"Could not find a face in the reference image.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: JRE.jpg not found. Please upload the reference image.\")\n",
        "        return # Exit if the reference image is not found\n",
        "    except IndexError:\n",
        "        print(\"Error processing reference image: No faces found or unexpected format.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Captura de video\n",
        "    # Using 0 for the default camera, may need to change depending on setup\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video stream.\")\n",
        "        return # Exit if camera cannot be opened\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame from video stream.\")\n",
        "            break\n",
        "\n",
        "        # Detectar personas usando YOLO\n",
        "        # Confidence threshold can be adjusted (conf=0.5 by default)\n",
        "        results = yolo_model(frame, classes=[0], verbose=False) # classes=[0] for person\n",
        "\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            for box in boxes:\n",
        "                # Ensure box coordinates are integers\n",
        "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
        "\n",
        "                # Extraer región de la persona, ensuring valid slice\n",
        "                if y1 >= 0 and y2 <= frame.shape[0] and x1 >= 0 and x2 <= frame.shape[1]:\n",
        "                    person_crop = frame[y1:y2, x1:x2]\n",
        "\n",
        "                    # Reconocimiento facial usando el modelo HOG en la región detectada por YOLO\n",
        "                    # Use the HOG model for face detection within the person crop\n",
        "                    face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
        "                    face_encodings = face_recognition.face_encodings(person_crop, face_locations, model=\"hog\")\n",
        "\n",
        "                    name = \"Desconocido\"\n",
        "                    # Compare detected faces within the crop to known faces\n",
        "                    for face_encoding in face_encodings:\n",
        "                        # Ensure known_face_encodings is not empty before comparing\n",
        "                        if known_face_encodings:\n",
        "                            matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.6)\n",
        "                            if True in matches:\n",
        "                                match_index = matches.index(True)\n",
        "                                name = known_face_names[match_index]\n",
        "                                break # Stop searching once a match is found\n",
        "\n",
        "                    # Dibujar resultados en el frame original\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    # Adjust text position to be inside the bounding box if it's too close to the top\n",
        "                    text_y_position = y1 - 10 if y1 - 10 > 10 else y1 + 10\n",
        "                    cv2.putText(frame, name, (x1, text_y_position), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "                else:\n",
        "                    print(f\"Warning: Invalid bounding box coordinates: ({x1}, {y1}, {x2}, {y2})\")\n",
        "\n",
        "\n",
        "        # Mostrar el frame\n",
        "        # This will likely not work in a typical Colab environment without a display\n",
        "        # Consider alternatives like saving frames or using a virtual display\n",
        "        try:\n",
        "            cv2.imshow('Person Identification', frame)\n",
        "            # Check for key press to exit\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        except cv2.error as e:\n",
        "            print(f\"Error displaying frame: {e}\")\n",
        "            print(\"cv2.imshow() may not work in this environment. Consider saving frames or using a virtual display.\")\n",
        "            break # Exit loop if display fails\n",
        "\n",
        "    # Liberar recursos\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c5QqIG8AjAr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
