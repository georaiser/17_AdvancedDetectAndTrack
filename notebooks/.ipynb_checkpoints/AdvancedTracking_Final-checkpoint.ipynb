{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUPHq0Se4QpT"
      },
      "source": [
        "### Advanced Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/python3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf4qa8MO4VFi",
        "outputId": "a67cede9-b1aa-42b5-cb45-3d36b91daa45"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDS1lQfswoGL",
        "outputId": "a4716f58-5824-4737-ad10-52e82110b328"
      },
      "outputs": [],
      "source": [
        "# !pip install cython_bbox\n",
        "# !pip install loguru\n",
        "# !pip install thop\n",
        "# !pip install lap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p37vPinX0Dkc",
        "outputId": "83160529-e041-4e41-ba3f-3979778e5b51"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/georaiser/YOLOX.git\n",
        "# %cd '/content/YOLOX'\n",
        "# %ls\n",
        "# !pip install -v -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "-CtWZozV4QpV",
        "outputId": "88e2da54-bfc9-44aa-ff53-c06c9a6f7c28"
      },
      "outputs": [],
      "source": [
        "# # # !pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# !pip install effdet\n",
        "# !pip install deep-sort-realtime\n",
        "# !pip install git+https://github.com/ultralytics/ultralytics.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "knYIU1vV4QpX"
      },
      "outputs": [],
      "source": [
        "# Import common requirements\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import csv\n",
        "#import clip\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ad82lf3z4QpY"
      },
      "outputs": [],
      "source": [
        "# Import Torch & Models requirements\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.models.detection import (\n",
        "    fasterrcnn_resnet50_fpn,\n",
        "    FasterRCNN_ResNet50_FPN_Weights,\n",
        ")\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from effdet import create_model\n",
        "\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from yolox.tracker.byte_tracker import BYTETracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ovkM9Hq74QpY"
      },
      "outputs": [],
      "source": [
        "# Import YOLOX requirements\n",
        "from yolox.exp import get_exp\n",
        "from yolox.utils import postprocess\n",
        "\n",
        "from yolox.data.data_augment import ValTransform\n",
        "from yolox.utils import fuse_model  # Optionally fuse model layers for better performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyggqtNW4QpY",
        "outputId": "bd94d1c4-c3fd-45a8-e7ab-f903207063df"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO  # YOLO11 uses the Ultralytics framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kIch2EUwbyj",
        "outputId": "0ff38e02-eda9-4ab9-bc16-6b81bc3786a9"
      },
      "outputs": [],
      "source": [
        "# %cd '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WquzhQGF4QpY"
      },
      "outputs": [],
      "source": [
        "def setup_device():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(0)  # Use first GPU\n",
        "        # Enable cudnn benchmarking for better performance\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPTIfN7i4QpY",
        "outputId": "1ff94184-e86e-4af4-c249-bee8d9999c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = setup_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qWcFRIf64QpZ"
      },
      "outputs": [],
      "source": [
        "# COCO 92 classes (including background)\n",
        "class_names_91 = [\"__background__\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "\"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\",\n",
        "\"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eyeglasses\",\n",
        "\"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
        "\"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\n",
        "\"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "\"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\", \"door\", \"tv\", \"laptop\", \"mouse\",\n",
        "\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\",\n",
        "\"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\", \"hairbrush\"]\n",
        "\n",
        "\n",
        "# COCO 80 classes\n",
        "class_names_80 = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "\"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\",\n",
        "\"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\",\n",
        "\"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
        "\"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\",\n",
        "\"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\",\n",
        "\"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\",\n",
        "\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
        "\"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_K5GxOH4Qpa"
      },
      "source": [
        "##### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MZvrD0dF4Qpa"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "class ModelConfig:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "# Video configuration\n",
        "class VideoProcessorConfig:\n",
        "    def __init__(self, input_path, output_path):\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "        self.frame_skip = 2  # Number of frames to skip between detections\n",
        "        self.custom_size = 800  # resizing to custom max size (mantain aspect ratio)    ...no implemented yet\n",
        "        self.custom_fps = None  # custom fps                                            ...no implemented yet\n",
        "        self.tracking_algorithm = \"ByteTrack\"  # choose DeepSort or ByteTrack\n",
        "        self.enable_tracking = True\n",
        "        self.enable_counter = True\n",
        "        self.enable_display = True # False for google colab\n",
        "        self.enable_save = True\n",
        "        self.enable_drawer = False  # False to load lines from json file and also for google colab\n",
        "\n",
        "\n",
        "class YOLOXConfig:\n",
        "    def __init__(self, model_name):\n",
        "        self.exp_file = None  # Set to path for custom model\n",
        "        self.ckpt_file = \"../checkpoints/model_weights/{}.pth\".format(model_name)\n",
        "        #self.ckpt_file = \"/content/drive/MyDrive/16_AdvancedTracking/weights/{}.pth\".format(model_name)\n",
        "        self.confthre = 0.5\n",
        "        self.nmsthre = 0.5\n",
        "\n",
        "\n",
        "class YOLOConfig:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_path = \"../checkpoints/model_weights/{}.pt\".format(model_name)\n",
        "        #self.model_path = \"/content/drive/MyDrive/16_AdvancedTracking/weights/{}.pt\".format(model_name)\n",
        "        self.conf_thres = 0.5\n",
        "        self.iou_thres = 0.5\n",
        "\n",
        "\n",
        "# Allowed classes\n",
        "class AllowedClasses:\n",
        "    def __init__(self, model_name):\n",
        "        if \"yolo\" in model_name:\n",
        "            self.class_names = class_names_80\n",
        "        elif \"faster\" or \"efficientdet\" in model_name:\n",
        "            self.class_names = class_names_91\n",
        "        else:\n",
        "            print(\"Model not supported\")\n",
        "            exit(0)\n",
        "\n",
        "    def get_allowed_classes(self):\n",
        "        self.allowed_classes = [\n",
        "            self.class_names.index(\"person\"),\n",
        "            self.class_names.index(\"car\"),\n",
        "            self.class_names.index(\"truck\"),\n",
        "            self.class_names.index(\"bus\"),\n",
        "        ]\n",
        "\n",
        "        # self.allowed_classes = [\n",
        "        #     self.class_names.index(\"suitcase\"),\n",
        "        #     self.class_names.index(\"backpack\"),\n",
        "        #     self.class_names.index(\"handbag\"),\n",
        "        # ]\n",
        "\n",
        "        #self.allowed_classes = list(range(0, len(self.class_names)))\n",
        "\n",
        "        return self.allowed_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v3Dhf5X44Qpa"
      },
      "outputs": [],
      "source": [
        "# Detection configuration\n",
        "class DetectorConfig:\n",
        "    def __init__(self, class_names, allowed_classes):\n",
        "        self.class_names = class_names\n",
        "        self.allowed_classes = allowed_classes\n",
        "        self.threshold = 0.5\n",
        "        self.iou_threshold = 0.5 # nms\n",
        "        self.nms_type = \"torchvision\"\n",
        "\n",
        "\n",
        "# Tracker configuration (DeepSort)\n",
        "class TrackerConfig_DeepSort:\n",
        "    def __init__(self):\n",
        "        self.max_age = 10  # Frames before track deletion\n",
        "        self.n_init = 3  # Frames needed to start track\n",
        "        self.nms_max_overlap = 0.5  # NMS threshold\n",
        "        self.max_cosine_distance = 0.3  # Feature matching threshold\n",
        "        self.nn_budget = None  # Maximum feature cache size\n",
        "        self.embedder = \"mobilenet\"  # Feature extractor. [\"mobilenet\",\"torchreid\",\"clip_RN50\",\"clip_RN101\",\"clip_RN50x4\",\n",
        "        # \"clip_RN50x16\", \"clip_ViT-B/32\",\"clip_ViT-B/16\",\"clip_ViT-L/14\"]\n",
        "        # print(clip.available_models())\n",
        "        self.half = True  # Use half precision for embeddings and cosine distance\n",
        "        self.bgr = True  # BGR input format\n",
        "        self.embedder_gpu = True  # Embedder GPU acceleration\n",
        "        self.today = None  # Today's date for logging\n",
        "\n",
        "# Tracker configuration (ByteTrack)\n",
        "class TrackerConfig_ByteTrack:\n",
        "    def __init__(self, frame_skip):\n",
        "        self.track_thresh = 0.5\n",
        "        self.track_buffer = 60\n",
        "        self.match_thresh = 0.6\n",
        "        self.frame_rate = (\n",
        "            30 / frame_skip\n",
        "        )  # fps -> fps_ori/frame_skip\n",
        "        self.mot20 = True\n",
        "# frame_rate=10 and track_buffer=30\n",
        "# -> object is tracked for 3 seconds before being removed\n",
        "\n",
        "\n",
        "# Drawer configuration\n",
        "class DrawerConfig:  #### REVISAR\n",
        "    def __init__(self):\n",
        "        self.line_thickness = 2\n",
        "        self.line_type = cv2.LINE_AA\n",
        "        self.color_palette = [\n",
        "            (0, 0, 255),  # Red in BGR\n",
        "            (255, 0, 0),  # Blue in BGR\n",
        "            (255, 0, 255),  # Magenta in BGR\n",
        "            (0, 165, 255),  # Orange in BGR\n",
        "            (0, 255, 255),  # Yellow in BGR\n",
        "            (128, 0, 128),  # Purple in BGR\n",
        "            (0, 128, 128),  # Olive in BGR\n",
        "            (0, 255, 0),  # Green in BGR\n",
        "        ]\n",
        "        self.lines = []\n",
        "\n",
        "\n",
        "# Counter configuration\n",
        "class CounterConfig:\n",
        "    def __init__(self):\n",
        "        self.text_color = (100, 10, 10)  # BGR format\n",
        "        self.text_scale = 0.4\n",
        "        self.text_thickness = 1\n",
        "        self.line_thickness = 2\n",
        "        self.text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        self.text_line_type = cv2.LINE_AA\n",
        "        self.line_spacing = 25\n",
        "        self.base_y_offset = 60\n",
        "        self.cell_width = 40\n",
        "        self.cell_height = 15\n",
        "        self.background_color = (240, 240, 210)  # BGR format\n",
        "\n",
        "# Display configuration\n",
        "class DisplayConfig:  # not all parameters are used\n",
        "    def __init__(self):\n",
        "        # Window settings\n",
        "        self.window_name = \"Object Detection\"\n",
        "        self.window_width = 1280\n",
        "        self.window_height = 720\n",
        "        self.fullscreen = False\n",
        "\n",
        "        # Box settings\n",
        "        self.box_color = (0, 255, 0)  # BGR format\n",
        "        self.box_thickness = 1\n",
        "        self.box_line_type = cv2.LINE_AA\n",
        "\n",
        "        # Text settings\n",
        "        self.text_color = (0, 0, 0)  # BGR format\n",
        "        self.text_scale = 0.4\n",
        "        self.text_thickness = 1\n",
        "        self.text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        self.text_line_type = cv2.LINE_AA\n",
        "        self.text_padding = 5  # Pixels above box\n",
        "\n",
        "        # Display options\n",
        "        self.show_fps = True\n",
        "        self.show_labels = True\n",
        "        self.show_confidence = True\n",
        "        self.show_tracking_id = True\n",
        "        self.show_class_name = True\n",
        "\n",
        "        # FPS display settings\n",
        "        self.fps_position = (10, 30)  # (x,y) coordinates\n",
        "        self.base_y_offset = 30\n",
        "        self.fps_color = (0, 0, 0)\n",
        "        self.fps_scale = 0.4\n",
        "\n",
        "        # Background settings\n",
        "        self.background_color = (200, 100, 200)  # BGR format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaW6Ezed4Qpa"
      },
      "source": [
        "##### Memory Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LMzE_02n4Qpb"
      },
      "outputs": [],
      "source": [
        "class MemoryManager:\n",
        "    def __init__(self, cleanup_frequency=100):\n",
        "        self.cleanup_frequency = cleanup_frequency\n",
        "\n",
        "    def cleanup(self, frame_count):\n",
        "        if frame_count % self.cleanup_frequency == 0:\n",
        "            if torch.cuda.is_available():\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "                # Force garbage collection\n",
        "                import gc\n",
        "                gc.collect()\n",
        "\n",
        "    @staticmethod\n",
        "    def print_memory_stats():\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "            print(f\"GPU Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCBgcRm4Qpb"
      },
      "source": [
        "#### Model Handler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2WrCtE54Qpb"
      },
      "source": [
        "##### YOLOX Handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DvCkVY514Qpb"
      },
      "outputs": [],
      "source": [
        "class YOLOXHandler:\n",
        "    \"\"\"Handles YOLOX model implementation based on official demo code\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, device, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "\n",
        "        # Extract config parameters with defaults\n",
        "        self.exp_file = kwargs.get(\"exp_file\", None)\n",
        "        self.ckpt_file = kwargs.get(\"ckpt_file\", None)\n",
        "        self.confthre = kwargs.get(\"confthre\", 0.5)\n",
        "        self.nmsthre = kwargs.get(\"nmsthre\", 0.5)\n",
        "        self.legacy = kwargs.get(\"legacy\", False)\n",
        "\n",
        "        # Initialize model from experiment file or model name\n",
        "        self.exp = get_exp(exp_file=self.exp_file, exp_name=self.model_name)\n",
        "        self.exp.test_conf = self.confthre\n",
        "        self.exp.nmsthre = self.nmsthre\n",
        "        self.num_classes = self.exp.num_classes\n",
        "        self.test_size = self.exp.test_size\n",
        "\n",
        "        # Load model\n",
        "        self.model = self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load YOLOX model based on experiment definition\"\"\"\n",
        "        model = self.exp.get_model()\n",
        "\n",
        "        # Load checkpoint\n",
        "        ckpt = torch.load(self.ckpt_file, map_location=self.device)\n",
        "\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "\n",
        "        model.to(self.device)\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Optionally for better performance\n",
        "        model = fuse_model(model)\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBYIJNyA4Qpb"
      },
      "source": [
        "##### YOLO Handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NwIBNsJ34Qpb"
      },
      "outputs": [],
      "source": [
        "class YOLOHandler:\n",
        "    \"\"\"Handles YOLO11 model implementation using Ultralytics framework\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, device, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "\n",
        "        # Extract config parameters with defaults\n",
        "        self.model_path = kwargs.get(\"model_path\", None)\n",
        "        self.conf_thres = kwargs.get(\"conf_thres\", 0.5)\n",
        "        self.iou_thres = kwargs.get(\"iou_thres\", 0.5)\n",
        "\n",
        "        # Load model\n",
        "        self.model = self._load_model()\n",
        "        # Device\n",
        "        self.model.to(self.device)\n",
        "        # Set to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load YOLO model using Ultralytics\"\"\"\n",
        "        model = YOLO(self.model_path)\n",
        "        model.to(self.device)\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odf5lRVa4Qpb"
      },
      "source": [
        "##### ClasicModel Handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sllLzJWr4Qpb"
      },
      "outputs": [],
      "source": [
        "class ClasicModelHandler:\n",
        "    def __init__(self, model_name, device):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.model = self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the specified model with appropriate configurations\"\"\"\n",
        "        model = None\n",
        "\n",
        "        if self.model_name.startswith(\"tf_efficientdet\"):\n",
        "            model = create_model(self.model_name, pretrained=True, bench_task=\"predict\")\n",
        "            model = model.to(self.device)\n",
        "            model.eval()\n",
        "\n",
        "        elif self.model_name.startswith(\"fasterrcnn\"):\n",
        "            model = fasterrcnn_resnet50_fpn(\n",
        "                weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT,\n",
        "                progress=True,\n",
        "                weights_backbone=ResNet50_Weights.DEFAULT,\n",
        "            )\n",
        "            model = model.to(self.device)\n",
        "            model.eval()\n",
        "\n",
        "            return model\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOf3trc-4Qpc"
      },
      "source": [
        "##### MODELManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wq7hrr9h4Qpc"
      },
      "outputs": [],
      "source": [
        "class MODELManager:\n",
        "    def __init__(self, model_name):\n",
        "        #super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.device = setup_device()\n",
        "        self.model = self._load_model()\n",
        "        self.image_size = self.get_image_size()\n",
        "        self.param = self.get_model_parameters(self.model)\n",
        "\n",
        "    def _load_model(self):\n",
        "\n",
        "        if self.model_name.startswith(\"tf_efficientdet\"):\n",
        "            model = ClasicModelHandler(self.model_name, self.device).model\n",
        "\n",
        "        elif self.model_name.startswith(\"fasterrcnn\"):\n",
        "            model = ClasicModelHandler(self.model_name, self.device).model\n",
        "\n",
        "        elif self.model_name.startswith(\"yolox\"):\n",
        "            yolox_config = YOLOXConfig(self.model_name)\n",
        "            model = YOLOXHandler(self.model_name, self.device, **yolox_config.__dict__)\n",
        "\n",
        "        elif self.model_name.startswith(\"yolov\"):\n",
        "            yolo_config = YOLOConfig(self.model_name)\n",
        "            model = YOLOHandler(self.model_name, self.device, **yolo_config.__dict__)\n",
        "\n",
        "        elif self.model_name.startswith(\"yolo11\"):\n",
        "            yolo_config = YOLOConfig(self.model_name)\n",
        "            model = YOLOHandler(self.model_name, self.device, **yolo_config.__dict__)\n",
        "\n",
        "        elif self.model_name.startswith(\"yolo12\"):\n",
        "            yolo_config = YOLOConfig(self.model_name)\n",
        "            model = YOLOHandler(self.model_name, self.device, **yolo_config.__dict__)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def get_model_parameters(self, model):\n",
        "        \"\"\"Verify and print model parameter information\"\"\"\n",
        "\n",
        "        try:\n",
        "            model_parameters = self.model.parameters()\n",
        "        except AttributeError:\n",
        "            model_parameters = self.model.model.parameters()\n",
        "\n",
        "        total_params = sum(p.numel() for p in model_parameters)\n",
        "        trained_params = sum(p.numel() for p in model_parameters if p.requires_grad)\n",
        "\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable parameters: {trained_params:,}\")\n",
        "\n",
        "        return total_params, trained_params\n",
        "\n",
        "    def get_image_size(self):\n",
        "        size_map = {\n",
        "            \"tf_efficientdet_d0\": 512,\n",
        "            \"tf_efficientdet_d1\": 640,\n",
        "            \"tf_efficientdet_d2\": 768,\n",
        "            \"tf_efficientdet_d3\": 896,\n",
        "            \"tf_efficientdet_d4\": 1024,\n",
        "            \"tf_efficientdet_d5\": 1280,\n",
        "            \"tf_efficientdet_d6\": 1280,\n",
        "            \"tf_efficientdet_d7\": 1536,\n",
        "            \"fasterrcnn_resnet50_fpn\": 1024,\n",
        "        }\n",
        "\n",
        "        # If model name starts with \"yolo\", return 640\n",
        "        if self.model_name.startswith(\"yolo\"):\n",
        "            return 640\n",
        "\n",
        "        # Return size from size_map or default to None\n",
        "        return size_map.get(self.model_name, None)\n",
        "\n",
        "    def get_model(self):\n",
        "        \"\"\"Return the loaded model\"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def get_model_name(self):\n",
        "        \"\"\"Return the loaded model name\"\"\"\n",
        "        return self.model_name\n",
        "\n",
        "    def get_device(self):\n",
        "        \"\"\"Return the current device\"\"\"\n",
        "        return self.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bafglNb4Qpc"
      },
      "source": [
        "#### Image Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uUHHmw-L4Qpc"
      },
      "outputs": [],
      "source": [
        "class ImageProcessor:\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dimensions(original_width, original_height, target_size):\n",
        "        \"\"\"Calculate new dimensions maintaining aspect ratio\"\"\"\n",
        "        if original_width > original_height:\n",
        "            new_height = int(target_size * original_height / original_width)\n",
        "            new_width = target_size\n",
        "            top_pad = (target_size - new_height) // 2\n",
        "            bottom_pad = target_size - new_height - top_pad\n",
        "            left_pad = right_pad = 0\n",
        "\n",
        "        else:\n",
        "            new_width = int(target_size * original_width / original_height)\n",
        "            new_height = target_size\n",
        "            left_pad = (target_size - new_width) // 2\n",
        "            right_pad = target_size - new_width - left_pad\n",
        "            top_pad = bottom_pad = 0\n",
        "\n",
        "        padding_info = {\n",
        "            \"top_pad\": top_pad,\n",
        "            \"bottom_pad\": bottom_pad,\n",
        "            \"left_pad\": left_pad,\n",
        "            \"right_pad\": right_pad,\n",
        "            \"original_size\": (original_width, original_height),\n",
        "            \"resized_size\": (new_width, new_height),\n",
        "            \"padded_size\": (target_size, target_size),\n",
        "        }\n",
        "\n",
        "        return padding_info\n",
        "\n",
        "    @staticmethod\n",
        "    def pad_to_square(image, padding_info):\n",
        "        if isinstance(image, Image.Image):\n",
        "            # Resize PIL image to maintain aspect ratio\n",
        "            resized_image = image.resize(padding_info[\"resized_size\"])\n",
        "            # Create new image with padding\n",
        "            padded_image = Image.new(\"RGB\", (padding_info[\"padded_size\"]), (0, 0, 0))\n",
        "            padded_image.paste(\n",
        "                resized_image, (padding_info[\"left_pad\"], padding_info[\"top_pad\"])\n",
        "            )\n",
        "        elif isinstance(image, np.ndarray):\n",
        "            # Resize numpy array image maintaining aspect ratio\n",
        "            resized_image = cv2.resize(image, padding_info[\"resized_size\"])\n",
        "            # Create padded image\n",
        "            padded_image = np.zeros(\n",
        "                (padding_info[\"padded_size\"][1], padding_info[\"padded_size\"][0], 3),\n",
        "                dtype=np.uint8,\n",
        "            )\n",
        "            # Place resized image in padded image\n",
        "            padded_image[\n",
        "                padding_info[\"top_pad\"] : padding_info[\"top_pad\"]\n",
        "                + resized_image.shape[0],\n",
        "                padding_info[\"left_pad\"] : padding_info[\"left_pad\"]\n",
        "                + resized_image.shape[1],\n",
        "            ] = resized_image\n",
        "\n",
        "            # Convert numpy array to PIL image\n",
        "            padded_image = cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Image must be either PIL Image or NumPy array\")\n",
        "\n",
        "        return resized_image, padded_image\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unpad_coordinates(coords, padding_info):\n",
        "        \"\"\"Adjusts boxes coordinates from padded space to original/resized space.\"\"\"\n",
        "        if len(coords) == 0:\n",
        "            return coords\n",
        "\n",
        "        # Convert numpy array to torch tensor if needed\n",
        "        if isinstance(coords, np.ndarray):\n",
        "            coords = torch.from_numpy(coords)\n",
        "\n",
        "        # Extract padding information\n",
        "        top_pad = padding_info[\"top_pad\"]\n",
        "        left_pad = padding_info[\"left_pad\"]\n",
        "        padded_size = padding_info[\"padded_size\"]\n",
        "        #output_size = padding_info[\"resized_size\"]\n",
        "        output_size = padding_info[\"original_size\"]\n",
        "\n",
        "        # Remove padding\n",
        "        adjusted_coords = coords.clone()\n",
        "        adjusted_coords[:, [0, 2]] -= left_pad  # x coordinates\n",
        "        adjusted_coords[:, [1, 3]] -= top_pad  # y coordinates\n",
        "\n",
        "        # Scale to resized dimensions\n",
        "        scale_x = output_size[0] / (padded_size[0] - 2 * left_pad)\n",
        "        scale_y = output_size[1] / (padded_size[1] - 2 * top_pad)\n",
        "\n",
        "        adjusted_coords[:, [0, 2]] *= scale_x\n",
        "        adjusted_coords[:, [1, 3]] *= scale_y\n",
        "\n",
        "        return adjusted_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bEULy3H4Qpc"
      },
      "source": [
        "#### Detector Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "753xH9d34Qpd"
      },
      "source": [
        "##### NMS Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p1PjMg9Y4Qpd"
      },
      "outputs": [],
      "source": [
        "class NMSFilter:\n",
        "    def __init__(self, iou_threshold, nms_type=\"torchvision\"):\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.nms_type = nms_type\n",
        "\n",
        "    def apply(self, boxes, scores):\n",
        "        if self.nms_type == \"torchvision\":\n",
        "            return self._nms_torchvision(boxes, scores)\n",
        "        if self.nms_type == \"opencv\":\n",
        "            return self._nms_opencv(boxes, scores)\n",
        "\n",
        "    def _nms_torchvision(self, boxes, scores):\n",
        "        keep = torchvision.ops.nms(boxes, scores, self.iou_threshold)\n",
        "        return boxes[keep], scores[keep]\n",
        "\n",
        "    def _nms_opencv(self, boxes, scores):\n",
        "        boxes = boxes.cpu().numpy()\n",
        "        scores = scores.cpu().numpy()\n",
        "\n",
        "        indices = cv2.dnn.NMSBoxes(\n",
        "            boxes.tolist(),\n",
        "            scores.tolist(),\n",
        "            score_threshold=0.3,\n",
        "            nms_threshold=self.iou_threshold,\n",
        "        )\n",
        "\n",
        "        if len(indices) > 0:\n",
        "            indices = indices.flatten()\n",
        "            filtered_boxes = boxes[indices]\n",
        "            filtered_scores = scores[indices]\n",
        "            return filtered_boxes, filtered_scores\n",
        "        return np.array([]), np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmztQVQ64Qpd"
      },
      "source": [
        "##### Detection Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6RF2GgCX4Qpd"
      },
      "outputs": [],
      "source": [
        "class DetectionFilter:\n",
        "    \"\"\"Handles filtering of detections based on scores and classes\"\"\"\n",
        "\n",
        "    def __init__(self, score_threshold, allowed_classes):\n",
        "        self.score_threshold = score_threshold\n",
        "        self.allowed_classes = allowed_classes\n",
        "\n",
        "    def filter_detections(self, boxes, scores, labels):\n",
        "        \"\"\"Filter detections based on score threshold and allowed classes\"\"\"\n",
        "        # Score threshold filtering\n",
        "        keep = scores > self.score_threshold\n",
        "        boxes = boxes[keep]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]\n",
        "\n",
        "        # Class filtering if specified\n",
        "        if self.allowed_classes:\n",
        "            class_mask = torch.tensor(\n",
        "                [label.item() in self.allowed_classes for label in labels],\n",
        "                dtype=torch.bool,\n",
        "                device=labels.device,\n",
        "            )\n",
        "\n",
        "            boxes = boxes[class_mask]\n",
        "            scores = scores[class_mask]\n",
        "            labels = labels[class_mask]\n",
        "\n",
        "        return boxes, scores, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9mFxcPm4Qpd"
      },
      "source": [
        "#### Model Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S8ytQkS4Qpd"
      },
      "source": [
        "##### YOLOX Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NjzFurae4Qpd"
      },
      "outputs": [],
      "source": [
        "class YOLOXDetector:\n",
        "    def __init__(self, model_handler, device, detector_config):\n",
        "        self.model_handler = model_handler\n",
        "        self.model = model_handler.model.model\n",
        "        self.device = device\n",
        "        self.detector_config = detector_config\n",
        "\n",
        "        self.val_preproc = ValTransform(False)\n",
        "\n",
        "        self.detection_filter = DetectionFilter(\n",
        "            score_threshold=self.detector_config.threshold,\n",
        "            allowed_classes=self.detector_config.allowed_classes,\n",
        "        )\n",
        "        self.nms_filter = NMSFilter(\n",
        "            self.detector_config.iou_threshold, self.detector_config.nms_type\n",
        "        )\n",
        "\n",
        "    def detection_pipeline(self, frame, padding_info):\n",
        "        # Preprocess image\n",
        "        image, resized_image = self.preprocess_pad(frame, padding_info)\n",
        "        # Preprocess image\n",
        "        img, img_info = self.preprocess(image)\n",
        "        # Inference\n",
        "        boxes, scores, labels = self.inference(img, img_info)\n",
        "        # Filter detections\n",
        "        boxes, scores, labels = self.filter_detections(boxes, scores, labels)\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = self.adjust_boxes(boxes, padding_info)\n",
        "\n",
        "        return boxes, scores, labels, frame\n",
        "\n",
        "    def preprocess_pad(self, frame, padding_info):\n",
        "        # Resize and pad image\n",
        "        resized_image, padded_image = ImageProcessor.pad_to_square(frame, padding_info)\n",
        "\n",
        "        # Convert to tensor\n",
        "        #img_tensor = F.to_tensor(padded_image).unsqueeze(0)\n",
        "\n",
        "        return padded_image, resized_image\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        \"\"\"Preprocess image for inference\"\"\"\n",
        "        img_info = {\"id\": 0}\n",
        "        height, width = img.shape[:2]\n",
        "        img_info[\"height\"] = height\n",
        "        img_info[\"width\"] = width\n",
        "        img_info[\"raw_img\"] = img\n",
        "\n",
        "        test_size = self.model_handler.model.test_size\n",
        "\n",
        "        ratio = min(test_size[0] / height, test_size[1] / width)\n",
        "        img_info[\"ratio\"] = ratio\n",
        "\n",
        "        img, _ = self.val_preproc(img, None, test_size)\n",
        "        img = torch.from_numpy(img).unsqueeze(0)\n",
        "        img = img.float().to(self.device)\n",
        "\n",
        "        return img, img_info\n",
        "\n",
        "    def inference(self, img_tensor, img_info):\n",
        "        \"\"\"Run inference with YOLOX model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(img_tensor)\n",
        "\n",
        "            outputs = postprocess(\n",
        "                outputs,\n",
        "                self.model_handler.model.num_classes,\n",
        "                self.model_handler.model.confthre,\n",
        "                self.model_handler.model.nmsthre,\n",
        "                class_agnostic=True\n",
        "            )\n",
        "\n",
        "        # #print(f\"Min confidence: {outputs[0][:, 4].min().item()}\")\n",
        "        # #print(f\"Max confidence: {outputs[0][:, 4].max().item()}\")\n",
        "\n",
        "        # Process output format to match the expected format in the system\n",
        "        if outputs[0] is not None:\n",
        "            output = outputs[0].cpu()\n",
        "            bboxes = output[:, 0:4]\n",
        "            # Scale back to original image dimensions\n",
        "            bboxes /= img_info[\"ratio\"]\n",
        "            scores = output[:, 4] * output[:, 5]\n",
        "            cls_ids = output[:, 6]\n",
        "\n",
        "            # Format for the tracking system\n",
        "            return bboxes, scores, cls_ids\n",
        "        else:\n",
        "            return torch.empty((0, 4)), torch.empty(0), torch.empty(0)\n",
        "\n",
        "    def filter_detections(self, boxes, scores, labels):\n",
        "        # filter threshold and classes\n",
        "        boxes, scores, labels = self.detection_filter.filter_detections(\n",
        "            boxes, scores, labels\n",
        "        )\n",
        "        # Apply NMS\n",
        "        #boxes, scores = self.nms_filter.apply(boxes, scores)\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def adjust_boxes(self, boxes, padding_info):\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = ImageProcessor.unpad_coordinates(boxes, padding_info)\n",
        "        return boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUEB0OK4Qpe"
      },
      "source": [
        "##### YOLO Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f4P_JMoU4Qpe"
      },
      "outputs": [],
      "source": [
        "class YOLODetector:\n",
        "    def __init__(self, model_handler, device, detector_config):\n",
        "        self.model_handler = model_handler\n",
        "        self.model = model_handler.model.model\n",
        "        self.device = device\n",
        "        self.detector_config = detector_config\n",
        "\n",
        "        self.detection_filter = DetectionFilter(\n",
        "            score_threshold=self.detector_config.threshold,\n",
        "            allowed_classes=self.detector_config.allowed_classes,\n",
        "        )\n",
        "        self.nms_filter = NMSFilter(\n",
        "            self.detector_config.iou_threshold, self.detector_config.nms_type\n",
        "        )\n",
        "\n",
        "    def detection_pipeline(self, frame, padding_info):\n",
        "        # Preprocess image\n",
        "        image, resized_image = self.preprocess_pad(frame, padding_info)\n",
        "         # Inference\n",
        "        predictions = self.inference(image)\n",
        "        # Parse detections\n",
        "        boxes, scores, labels = self.parse_detections(predictions)\n",
        "        # Filter detections\n",
        "        boxes, scores, labels = self.filter_detections(boxes, scores, labels)\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = self.adjust_boxes(boxes, padding_info)\n",
        "\n",
        "        return boxes, scores, labels, frame\n",
        "\n",
        "    def preprocess_pad(self, frame, padding_info):\n",
        "        # Resize and pad image\n",
        "        resized_image, padded_image = ImageProcessor.pad_to_square(frame, padding_info)\n",
        "\n",
        "        return padded_image, resized_image\n",
        "\n",
        "    def inference(self, img):\n",
        "        # Run YOLO11 inference\n",
        "        results = self.model(\n",
        "            img,\n",
        "            conf=self.detector_config.threshold,\n",
        "            iou=self.detector_config.iou_threshold,\n",
        "            verbose=False,\n",
        "        )\n",
        "        return results[0]\n",
        "\n",
        "    def parse_detections(self, results):\n",
        "        # Extract boxes, confidence scores, and class IDs from YOLO11 results\n",
        "\n",
        "        boxes = results.boxes.xyxy\n",
        "        scores = results.boxes.conf\n",
        "        labels = results.boxes.cls\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def filter_detections(self, boxes, scores, labels):\n",
        "        # Filter based on confidence and allowed classes\n",
        "        boxes, scores, labels = self.detection_filter.filter_detections(\n",
        "            boxes, scores, labels\n",
        "        )\n",
        "        # Apply NMS\n",
        "        # boxes, scores = self.nms_filter.apply(boxes, scores)\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def adjust_boxes(self, boxes, padding_info):\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = ImageProcessor.unpad_coordinates(boxes, padding_info)\n",
        "        return boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPgEjtKB4Qpe"
      },
      "source": [
        "##### FasterRCNNDetector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8QHnBaKr4Qpe"
      },
      "outputs": [],
      "source": [
        "class FasterRCNNDetector:\n",
        "    def __init__(self, model_handler, device, detector_config):\n",
        "        self.model = model_handler.get_model()\n",
        "        self.device = device\n",
        "        self.detector_config = detector_config\n",
        "\n",
        "        self.detection_filter = DetectionFilter(\n",
        "            score_threshold=self.detector_config.threshold,\n",
        "            allowed_classes=self.detector_config.allowed_classes,\n",
        "        )\n",
        "        self.nms_filter = NMSFilter(\n",
        "            self.detector_config.iou_threshold, self.detector_config.nms_type\n",
        "        )\n",
        "\n",
        "    def detection_pipeline(self, frame, padding_info):\n",
        "        # Preprocess image\n",
        "        img_tensor, resized_image = self.preprocess(frame, padding_info)\n",
        "        # Inference\n",
        "        predictions = self.inference(img_tensor)\n",
        "        # Parse detections\n",
        "        boxes, scores, labels = self.parse_detections(predictions)\n",
        "        # Filter detections\n",
        "        boxes, scores, labels = self.filter_detections(boxes, scores, labels)\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = self.adjust_boxes(boxes, padding_info)\n",
        "\n",
        "        return boxes, scores, labels, resized_image\n",
        "\n",
        "    def preprocess(self, frame, padding_info):\n",
        "        # Resize and pad image\n",
        "        resized_image, padded_image = ImageProcessor.pad_to_square(frame, padding_info)\n",
        "\n",
        "        # Convert to tensor\n",
        "        img_tensor = F.to_tensor(padded_image).unsqueeze(0)\n",
        "\n",
        "        return img_tensor, resized_image\n",
        "\n",
        "    def inference(self, img_tensor):\n",
        "        # Ensure input tensor is on correct device\n",
        "        img_tensor = img_tensor.to(self.device)\n",
        "        # Ensure model is on correct device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # model inference\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(img_tensor)\n",
        "        return predictions\n",
        "\n",
        "    def parse_detections(self, predictions):\n",
        "        boxes = predictions[0][\"boxes\"]\n",
        "        scores = predictions[0][\"scores\"]\n",
        "        labels = predictions[0][\"labels\"]\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def filter_detections(self, boxes, scores, labels):\n",
        "        # filter threshold and classes\n",
        "        boxes, scores, labels = self.detection_filter.filter_detections(\n",
        "            boxes, scores, labels)\n",
        "        # Apply NMS\n",
        "        boxes, scores = self.nms_filter.apply(boxes, scores)\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def adjust_boxes(self, boxes, padding_info):\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = ImageProcessor.unpad_coordinates(boxes, padding_info)\n",
        "        return boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCtsMp_v4Qpe"
      },
      "source": [
        "##### EfficientDetDetector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DyLIlyx14Qpf"
      },
      "outputs": [],
      "source": [
        "class EfficientDetDetector:\n",
        "    def __init__(self, model_handler, device, detector_config):\n",
        "        self.model = model_handler.get_model()\n",
        "        self.device = device\n",
        "        self.detector_config = detector_config\n",
        "\n",
        "        self.detection_filter = DetectionFilter(\n",
        "            score_threshold=self.detector_config.threshold,\n",
        "            allowed_classes=self.detector_config.allowed_classes,\n",
        "        )\n",
        "        self.nms_filter = NMSFilter(\n",
        "            self.detector_config.iou_threshold, self.detector_config.nms_type\n",
        "        )\n",
        "\n",
        "    def detection_pipeline(self, frame, padding_info):\n",
        "        # Preprocess image\n",
        "        img_tensor, resized_image = self.preprocess(frame, padding_info)\n",
        "        # Inference\n",
        "        predictions = self.inference(img_tensor)\n",
        "        # Parse detections\n",
        "        boxes, scores, labels = self.parse_detections(predictions)\n",
        "\n",
        "        # print(f\"Min confidence: {scores.min().item()}\")\n",
        "        # print(f\"Max confidence: {scores.max().item()}\")\n",
        "\n",
        "        # Filter detections\n",
        "        boxes, scores, labels = self.filter_detections(boxes, scores, labels)\n",
        "\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = self.adjust_boxes(boxes, padding_info)\n",
        "\n",
        "        return boxes, scores, labels, resized_image\n",
        "\n",
        "    def preprocess(self, frame, padding_info):\n",
        "        # Resize and pad image\n",
        "        resized_image, padded_image = ImageProcessor.pad_to_square(frame, padding_info)\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        img_tensor = F.to_tensor(padded_image).unsqueeze(0)\n",
        "\n",
        "        # normalize\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        img_tensor = F.normalize(img_tensor, mean=mean, std=std)\n",
        "\n",
        "        return img_tensor, resized_image\n",
        "\n",
        "    def inference(self, img_tensor):\n",
        "        # Ensure input tensor is on correct device\n",
        "        img_tensor = img_tensor.to(self.device)\n",
        "        # Ensure model is on correct device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # model inference\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(img_tensor)\n",
        "        return predictions\n",
        "\n",
        "    def parse_detections(self, predictions):\n",
        "        boxes = predictions[0][:, :4]\n",
        "        scores = predictions[0][:, 4]\n",
        "        labels = predictions[0][:, 5].long()\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def filter_detections(self, boxes, scores, labels):\n",
        "        # filter threshold and classes\n",
        "        boxes, scores, labels = self.detection_filter.filter_detections(\n",
        "            boxes, scores, labels)\n",
        "        # Apply NMS\n",
        "        boxes, scores = self.nms_filter.apply(boxes, scores)\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def adjust_boxes(self, boxes, padding_info):\n",
        "        # Adjust boxes coordinates\n",
        "        boxes = ImageProcessor.unpad_coordinates(boxes, padding_info)\n",
        "        return boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebgblTf4Qpg"
      },
      "source": [
        "##### Detector Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S9J68KQc4Qpg"
      },
      "outputs": [],
      "source": [
        "class DetectorManager:\n",
        "    def __init__(self, model_handler, detector_config):\n",
        "        self.detector_config = detector_config\n",
        "        self.model_handler  = model_handler\n",
        "\n",
        "        try:\n",
        "            # yolo\n",
        "            self.model          = model_handler.model.model\n",
        "            self.model_name     = model_handler.model.model_name\n",
        "            self.device         = model_handler.model.device\n",
        "        except AttributeError:\n",
        "            # fasterrcnn / efficiendet\n",
        "            self.model          = model_handler.get_model()\n",
        "            self.model_name     = model_handler.get_model_name()\n",
        "            self.device         = model_handler.get_device()\n",
        "\n",
        "    def get_detector(self):\n",
        "        detector_map = {\n",
        "            \"tf_efficientdet\": EfficientDetDetector,\n",
        "            \"fasterrcnn\": FasterRCNNDetector,\n",
        "            \"yolox\": YOLOXDetector,\n",
        "            \"yolov5\": YOLODetector,\n",
        "            \"yolov8\": YOLODetector,\n",
        "            \"yolo11\": YOLODetector,\n",
        "            \"yolo12\": YOLODetector,\n",
        "        }\n",
        "\n",
        "        for key, detector_class in detector_map.items():\n",
        "            if self.model_name.startswith(key):\n",
        "                args = (self.model_handler, self.device, self.detector_config)\n",
        "                return detector_class(*args)\n",
        "\n",
        "        raise ValueError(f\"Unsupported model: {self.model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u90oIRX4Qpg"
      },
      "source": [
        "#### Tracker Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "njxPh4X74Qpg"
      },
      "outputs": [],
      "source": [
        "class Tracker_DeepSort:\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Handles object tracking using DeepSORT algorithm\"\"\"\n",
        "        self.tracker_config = config\n",
        "        self.tracker = DeepSort(**self.tracker_config.__dict__)\n",
        "\n",
        "    def update(self, frame, boxes, scores, labels):\n",
        "        # Convert detections to DeepSORT format - [x1,y1,w,h]\n",
        "        detections = []\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            detections.append(([x1, y1, w, h], score.item(), label.item()))\n",
        "\n",
        "        # Update tracks\n",
        "        tracks = self.tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        return tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "I-aLwR7_4Qpg"
      },
      "outputs": [],
      "source": [
        "class Tracker_ByteTrack:\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Handles object tracking using BYTETrack algorithm\"\"\"\n",
        "        self.tracker = BYTETracker(config, config.frame_rate)\n",
        "        self.track_labels = {}\n",
        "\n",
        "    def update(self, frame, boxes, scores, labels):\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        height, width = frame.shape[:2]\n",
        "        img_info = [height, width]\n",
        "        img_size = [height, width]\n",
        "\n",
        "        detections = []\n",
        "        temp_labels = []\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            detections.append([x1, y1, x2, y2, score.item()])\n",
        "            temp_labels.append(label.item())\n",
        "\n",
        "        detections = np.array(detections)\n",
        "\n",
        "        tracks = self.tracker.update(detections, img_info, img_size)\n",
        "\n",
        "        # Get matching indices using scores by JRE\n",
        "        temp_labels_matched = []\n",
        "\n",
        "        scores_cpu = []\n",
        "        for score in scores:\n",
        "            score = score.cpu().item()\n",
        "            scores_cpu.append(score)\n",
        "\n",
        "        for track in tracks:\n",
        "            track_score = track.score  # Get track score\n",
        "            matched_idx = np.where(np.equal(track_score, scores_cpu))[0]\n",
        "\n",
        "            if len(matched_idx) > 0:\n",
        "                idx = matched_idx[0]\n",
        "                temp_labels_matched.append(temp_labels[idx])\n",
        "\n",
        "        for track, label in zip(tracks, temp_labels_matched):\n",
        "            if track.track_id in self.track_labels:\n",
        "                if track.score > self.track_labels[track.track_id][1]:\n",
        "                    self.track_labels[track.track_id] = [label, track.score]\n",
        "                    track.label = label\n",
        "            else:\n",
        "                self.track_labels[track.track_id] = [label, track.score]\n",
        "                track.label = label\n",
        "\n",
        "        return tracks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghUmuGfG4Qpg"
      },
      "source": [
        "##### TrackerManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lY0U6M_c4Qpg"
      },
      "outputs": [],
      "source": [
        "class TrackerManager:\n",
        "    def __init__(self, algorithm):\n",
        "        self.algorithm = algorithm\n",
        "\n",
        "        if \"DeepSort\" in self.algorithm.__class__.__name__:\n",
        "            self.tracker_processor = Tracker_DeepSort(self.algorithm)\n",
        "        elif \"ByteTrack\" in self.algorithm.__class__.__name__:\n",
        "            self.tracker_processor = Tracker_ByteTrack(self.algorithm)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported tracking algorithm: {self.algorithm}\")\n",
        "\n",
        "    def update(self, frame, boxes, scores, labels):\n",
        "        return self.tracker_processor.update(frame, boxes, scores, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIyiIEFb4Qph"
      },
      "source": [
        "#### LineDrawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PAWzIF3u4Qph"
      },
      "outputs": [],
      "source": [
        "class LineDrawer:\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Initialize the LineDrawer class\"\"\"\n",
        "        self.config = config\n",
        "\n",
        "        self.video_path = config.video_config.input_path\n",
        "        self.color_palette = config.drawer_config.color_palette\n",
        "\n",
        "        self.lines_config = []\n",
        "        self.frame_with_lines = None\n",
        "        self.temp_start_point = None\n",
        "        self.line_counter = 1\n",
        "\n",
        "        self._load_video_frame()\n",
        "\n",
        "    def _load_video_frame(self, frame_number=100):\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "\n",
        "        # Skip frames\n",
        "        for _ in range(frame_number):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "                ret, frame = cap.read()\n",
        "                break\n",
        "        cap.release()\n",
        "\n",
        "        self.frame = frame\n",
        "\n",
        "        if not ret:\n",
        "            raise RuntimeError(f\"Failed to read the video: {self.video_path}\")\n",
        "\n",
        "    def _draw_line(self, event, x, y, flags, param):\n",
        "        \"\"\"\n",
        "        Callback function to handle mouse events and draw lines.\n",
        "\n",
        "        :param event: Mouse event.\n",
        "        :param x: X-coordinate of the event.\n",
        "        :param y: Y-coordinate of the event.\n",
        "        :param flags: Additional flags for the event.\n",
        "        :param param: Additional parameters.\n",
        "        \"\"\"\n",
        "        if event == cv2.EVENT_LBUTTONDOWN:  # On left mouse button click\n",
        "            if self.temp_start_point is None:\n",
        "                # Store the first point of the line\n",
        "                self.temp_start_point = (x, y)\n",
        "            else:\n",
        "                # Store the second point, draw the line, and reset\n",
        "                temp_end_point = (x, y)\n",
        "\n",
        "                # Use a color from the palette\n",
        "                color = self.color_palette[(self.line_counter - 1) % len(self.color_palette)]\n",
        "\n",
        "                # Define the line configuration\n",
        "                line_config = {\n",
        "                    'start_point': self.temp_start_point,\n",
        "                    'end_point': temp_end_point,\n",
        "                    'name': f'Line{self.line_counter}',\n",
        "                    'color': color\n",
        "                }\n",
        "\n",
        "                self.lines_config.append(line_config)\n",
        "                self.line_counter += 1\n",
        "\n",
        "                # Draw the line on the frame\n",
        "                cv2.line(self.frame, line_config['start_point'], line_config['end_point'], color,\n",
        "                         self.config.drawer_config.line_thickness)\n",
        "                cv2.imshow(\"Draw Lines\", self.frame)\n",
        "\n",
        "                # Reset the start point\n",
        "                self.temp_start_point = None\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Start the line drawing interaction.\n",
        "        \"\"\"\n",
        "        cv2.namedWindow(\"Draw Lines\")\n",
        "        cv2.setMouseCallback(\"Draw Lines\", self._draw_line)\n",
        "\n",
        "        print(\"Click to define points for lines (two clicks per line). Press 'q' to quit.\")\n",
        "        while True:\n",
        "            cv2.imshow(\"Draw Lines\", self.frame)\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if key == ord(\"q\"):  # Press 'q' to quit\n",
        "                break\n",
        "            # Saving the frame\n",
        "            cv2.imwrite('frame.jpg', self.frame)\n",
        "\n",
        "        cv2.destroyAllWindows()\n",
        "        # Save to JSON file\n",
        "        with open(\"lines_geometry.json\", \"w\") as file:\n",
        "            json.dump(self.lines_config, file, indent=4)\n",
        "\n",
        "        return self.lines_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuKs3G724Qph"
      },
      "source": [
        "#### Counter Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsBkosJ24Qph"
      },
      "source": [
        "##### Geometry Calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FfX4xUSt4Qph"
      },
      "outputs": [],
      "source": [
        "class GeometryCalculator:\n",
        "    \"\"\"Utility class for geometric calculations related to line crossing\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_line_side(point, line_start, line_end):\n",
        "        \"\"\"\n",
        "        Compute which side of a line a point is on.\n",
        "        Returns a normalized value: positive = one side, negative = other side\n",
        "        \"\"\"\n",
        "        x, y = point\n",
        "        x1, y1 = line_start\n",
        "        x2, y2 = line_end\n",
        "        line_length = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
        "\n",
        "        if line_length == 0:\n",
        "            return 0\n",
        "\n",
        "        # Normalized cross product\n",
        "        return ((x2 - x1) * (y - y1) - (x - x1) * (y2 - y1)) / line_length\n",
        "\n",
        "    @staticmethod\n",
        "    def is_point_near_line(point, line_start, line_end, threshold):\n",
        "        \"\"\"\n",
        "        Check if a point is within a threshold distance of a line segment\n",
        "        \"\"\"\n",
        "        x, y = point\n",
        "        x1, y1 = line_start\n",
        "        x2, y2 = line_end\n",
        "\n",
        "        # Vector math for accurate distance calculation\n",
        "        line_vec = np.array([x2 - x1, y2 - y1])\n",
        "        point_vec = np.array([x - x1, y - y1])\n",
        "\n",
        "        line_length = np.linalg.norm(line_vec)\n",
        "\n",
        "        if line_length == 0:\n",
        "            return False\n",
        "\n",
        "        line_unit_vec = line_vec / line_length\n",
        "        projection_length = np.dot(point_vec, line_unit_vec)\n",
        "\n",
        "        # Check if projection is on the line segment\n",
        "        if 0 <= projection_length <= line_length:\n",
        "            distance = abs(np.cross(line_unit_vec, point_vec))\n",
        "            return distance < threshold\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def get_bbox_center(bbox):\n",
        "        \"\"\"Calculate center point of a bounding box [x1, y1, x2, y2]\"\"\"\n",
        "        return ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_adaptive_threshold(bbox):\n",
        "        \"\"\"Calculate an adaptive threshold based on object size\"\"\"\n",
        "        width = bbox[2] - bbox[0]\n",
        "        height = bbox[3] - bbox[1]\n",
        "        return min(width, height) * 1.5  # Adjust multiplier as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELg1ocJx4Qpi"
      },
      "source": [
        "##### Line Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MZhtCx224Qpi"
      },
      "outputs": [],
      "source": [
        "class Line:\n",
        "    \"\"\"Represents a counting line with associated tracking state\"\"\"\n",
        "    def __init__(self, lines_geometry):\n",
        "        self.start_point = lines_geometry[\"start_point\"]\n",
        "        self.end_point = lines_geometry[\"end_point\"]\n",
        "        self.name = lines_geometry.get(\"name\") or f\"Line-{id(self)}\"\n",
        "        self.color = lines_geometry.get(\"color\")\n",
        "        # Counter state\n",
        "        self.counts = {\"up\": {}, \"down\": {}}\n",
        "        self.tracked_objects = {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjGjIURU4Qpi"
      },
      "source": [
        "##### Object Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W9quLLVk4Qpi"
      },
      "outputs": [],
      "source": [
        "class ObjectCounter:\n",
        "    \"\"\"\n",
        "    Tracks objects crossing lines and maintains counts by direction and object class\n",
        "    \"\"\"\n",
        "    def __init__(self, allowed_classes=None, class_names=None):\n",
        "        self.allowed_classes = set(allowed_classes or [])\n",
        "        self.class_names = class_names or []\n",
        "        self.geometry = GeometryCalculator()\n",
        "\n",
        "    def update(self, tracks, lines):\n",
        "        \"\"\"\n",
        "        Update object counts based on current tracked objects\n",
        "        \"\"\"\n",
        "        for line in lines:\n",
        "            self._process_line_crossings(line, tracks)\n",
        "\n",
        "    def _process_line_crossings(self, line, tracks):\n",
        "        \"\"\"Process all tracks for a specific line\"\"\"\n",
        "        for track in tracks:\n",
        "            # Skip if not a valid track or not in allowed classes\n",
        "            if not self._is_valid_track(track):\n",
        "                continue\n",
        "\n",
        "            # Get the bounding box and center point\n",
        "            bbox = self._get_track_bbox(track)\n",
        "            if bbox is None:\n",
        "                continue\n",
        "\n",
        "            center = self.geometry.get_bbox_center(bbox)\n",
        "            threshold = self.geometry.get_adaptive_threshold(bbox)\n",
        "\n",
        "            # Skip if not near the line\n",
        "            if not self.geometry.is_point_near_line(\n",
        "                center, line[\"start_point\"], line[\"end_point\"], threshold\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            # Process the crossing\n",
        "            self._update_crossing_count(line, track, center)\n",
        "\n",
        "    def _is_valid_track(self, track):\n",
        "        \"\"\"Check if track is valid and belongs to allowed classes\"\"\"\n",
        "        # Check if track is confirmed - different trackers use different methods\n",
        "        if hasattr(track, \"is_confirmed\") and callable(getattr(track, \"is_confirmed\")):\n",
        "            is_confirmed = track.is_confirmed()\n",
        "        elif hasattr(track, \"is_activated\"):\n",
        "            is_confirmed = track.is_activated\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "        # Check class - different trackers use different attributes\n",
        "        if hasattr(track, \"det_class\"):\n",
        "            class_id = track.det_class\n",
        "        elif hasattr(track, \"label\"):\n",
        "            class_id = track.label\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "        # Track must be confirmed and in allowed classes (if specified)\n",
        "        return is_confirmed and (\n",
        "            not self.allowed_classes or class_id in self.allowed_classes\n",
        "        )\n",
        "\n",
        "    def _get_track_bbox(self, track):\n",
        "        \"\"\"Get the bounding box from a track object, handling different formats\"\"\"\n",
        "        if hasattr(track, \"to_ltrb\") and callable(getattr(track, \"to_ltrb\")):\n",
        "            # DeepSORT format\n",
        "            return track.to_ltrb()\n",
        "        elif hasattr(track, \"tlwh\"):\n",
        "            # ByteTrack format\n",
        "            l, t, w, h = track.tlwh  # noqa: E741\n",
        "            return [l, t, l + w, t + h]\n",
        "        return None\n",
        "\n",
        "    def _update_crossing_count(self, line, track, center):\n",
        "        \"\"\"Update crossing counts for a single track\"\"\"\n",
        "        track_id = track.track_id\n",
        "\n",
        "        # Get class ID according to tracker type\n",
        "        class_id = track.det_class if hasattr(track, \"det_class\") else track.label\n",
        "\n",
        "        # Get current side of the line\n",
        "        current_side = self.geometry.compute_line_side(\n",
        "            center, line[\"start_point\"], line[\"end_point\"]\n",
        "        )\n",
        "\n",
        "        # Initialize tracking state for new objects\n",
        "        if track_id not in line[\"tracked_objects\"]:\n",
        "            line[\"tracked_objects\"][track_id] = {\n",
        "                \"prev_side\": current_side,\n",
        "                \"class\": class_id,\n",
        "                \"counted\": False,\n",
        "            }\n",
        "            return\n",
        "\n",
        "        # Get the current state for this object\n",
        "        current_state = line[\"tracked_objects\"][track_id]\n",
        "\n",
        "        # Check if the object has crossed the line\n",
        "        if (\n",
        "            not current_state[\"counted\"]\n",
        "            and current_state[\"prev_side\"] * current_side <= 0\n",
        "        ):\n",
        "            # Determine direction of crossing (based on sign of current side)\n",
        "            direction = \"down\" if current_side > 0 else \"up\"\n",
        "\n",
        "            # Initialize counter if needed\n",
        "            if class_id not in line[\"counts\"][direction]:\n",
        "                line[\"counts\"][direction][class_id] = 0\n",
        "\n",
        "            # Increment count\n",
        "            line[\"counts\"][direction][class_id] += 1\n",
        "\n",
        "            # Mark as counted to prevent multiple counts for same crossing\n",
        "            current_state[\"counted\"] = True\n",
        "\n",
        "        # Update previous side\n",
        "        current_state[\"prev_side\"] = current_side\n",
        "\n",
        "    def get_counts(self):\n",
        "        \"\"\"Get counts for all lines\"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"name\": line[\"name\"],\n",
        "                \"up\": line[\"counts\"][\"up\"],\n",
        "                \"down\": line[\"counts\"][\"down\"],\n",
        "                \"total_up\": sum(line[\"counts\"][\"up\"].values()),\n",
        "                \"total_down\": sum(line[\"counts\"][\"down\"].values()),\n",
        "            }\n",
        "            for line in self.lines\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTWibDaH4Qpi"
      },
      "source": [
        "##### Counter Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bztpCWnZ4Qpj"
      },
      "outputs": [],
      "source": [
        "class CounterVisualizer:\n",
        "    \"\"\"Handles visualization of counter lines and statistics\"\"\"\n",
        "\n",
        "    def __init__(self, counter, class_names, config):\n",
        "        self.counter = counter\n",
        "        self.class_names = class_names\n",
        "        self.config = config.counter_config\n",
        "\n",
        "    def draw(self, frame, lines):\n",
        "        \"\"\"Draw all lines and statistics on the frame\"\"\"\n",
        "        # Draw lines first\n",
        "        for line in lines:\n",
        "            cv2.line(\n",
        "                frame,\n",
        "                line[\"start_point\"],\n",
        "                line[\"end_point\"],\n",
        "                line[\"color\"],\n",
        "                self.config.line_thickness,\n",
        "            )\n",
        "        # Draw statistics\n",
        "        self._draw_statistics(frame, lines)\n",
        "        return frame\n",
        "\n",
        "    def _draw_statistics(self, frame, lines):\n",
        "        \"\"\"Draw crossing statistics for all lines\"\"\"\n",
        "        # Collect all unique classes across all lines\n",
        "        all_classes = set()\n",
        "        for line in lines:\n",
        "            for direction in [\"up\", \"down\"]:\n",
        "                all_classes.update(line[\"counts\"][direction].keys())\n",
        "\n",
        "        # Sort classes for consistent ordering\n",
        "        sorted_classes = sorted(list(all_classes))\n",
        "\n",
        "        # Base vertical offset\n",
        "        y_offset = self.config.base_y_offset\n",
        "\n",
        "        for line in lines:\n",
        "            # Calculate totals\n",
        "            total_up = sum(line[\"counts\"][\"up\"].values())\n",
        "            total_down = sum(line[\"counts\"][\"down\"].values())\n",
        "            line_summary = f\"{line['name']}: Up: {total_up}, Down: {total_down}\"\n",
        "\n",
        "            # Get text size for background\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(\n",
        "                line_summary,\n",
        "                self.config.text_font,\n",
        "                self.config.text_scale,\n",
        "                self.config.text_thickness\n",
        "            )\n",
        "\n",
        "            # Draw background\n",
        "            cv2.rectangle(\n",
        "                frame,\n",
        "                (10, y_offset - text_height - baseline),\n",
        "                (10 + text_width, y_offset + baseline),\n",
        "                self.config.background_color,\n",
        "                cv2.FILLED,\n",
        "            )\n",
        "\n",
        "            # Draw text\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                line_summary,\n",
        "                (10, y_offset),\n",
        "                self.config.text_font,\n",
        "                self.config.text_scale,\n",
        "                line[\"color\"],\n",
        "                self.config.text_thickness,\n",
        "                self.config.text_line_type,\n",
        "            )\n",
        "\n",
        "            # Move to next line\n",
        "            y_offset += self.config.line_spacing\n",
        "\n",
        "            # Draw class-specific counts\n",
        "            if sorted_classes:\n",
        "                self._draw_class_counts(frame, line, sorted_classes, y_offset)\n",
        "\n",
        "                # Move to next line section\n",
        "                y_offset += self.config.cell_height * 3 + 10\n",
        "\n",
        "    def _draw_class_counts(self, frame, line, classes, y_offset):\n",
        "        \"\"\"Draw class-specific counts for a line\"\"\"\n",
        "        x_start = 10\n",
        "        cell_width = self.config.cell_width\n",
        "        cell_height = self.config.cell_height\n",
        "\n",
        "        # Draw column headers (classes)\n",
        "        for class_idx, class_id in enumerate(classes):\n",
        "            class_name = (\n",
        "                self.class_names[int(class_id)]\n",
        "                if class_id < len(self.class_names)\n",
        "                else f\"Class {int(class_id)}\"\n",
        "            )\n",
        "\n",
        "            # Get text size for proper background sizing\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(\n",
        "                class_name,\n",
        "                self.config.text_font,\n",
        "                self.config.text_scale,\n",
        "                self.config.text_thickness\n",
        "            )\n",
        "\n",
        "            # Draw background\n",
        "            cv2.rectangle(\n",
        "                frame,\n",
        "                (x_start + (class_idx + 1) * cell_width, y_offset - 15),\n",
        "                (x_start + (class_idx + 2) * cell_width, y_offset + baseline),\n",
        "                self.config.background_color,\n",
        "                cv2.FILLED,\n",
        "            )\n",
        "\n",
        "            # Draw class name\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                class_name,\n",
        "                (x_start + (class_idx + 1) * cell_width + 5, y_offset - 2),\n",
        "                self.config.text_font,\n",
        "                self.config.text_scale,\n",
        "                (0, 0, 0),\n",
        "                self.config.text_thickness,\n",
        "                self.config.text_line_type,\n",
        "            )\n",
        "\n",
        "        y_offset += cell_height\n",
        "\n",
        "        # Draw direction rows\n",
        "        directions = [(\"Up\", (0, 100, 200)), (\"Down\", (0, 100, 200))]\n",
        "\n",
        "        for direction, color in directions:\n",
        "            # Draw direction label up/down\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                direction,\n",
        "                (x_start, y_offset - 2),\n",
        "                self.config.text_font,\n",
        "                self.config.text_scale,\n",
        "                color,\n",
        "                self.config.text_thickness,\n",
        "                self.config.text_line_type\n",
        "            )\n",
        "\n",
        "            # Draw counts for each class\n",
        "            for class_idx, class_id in enumerate(classes):\n",
        "                count = line[\"counts\"][direction.lower()].get(class_id, 0)\n",
        "                count_text = str(count)\n",
        "\n",
        "                # Get text size for centering\n",
        "                (text_width, _), _ = cv2.getTextSize(\n",
        "                    count_text,\n",
        "                    self.config.text_font,\n",
        "                    self.config.text_scale,\n",
        "                    self.config.text_thickness\n",
        "                )\n",
        "\n",
        "                # Draw background\n",
        "                cv2.rectangle(\n",
        "                    frame,\n",
        "                    (x_start + (class_idx + 1) * cell_width, y_offset - cell_height),\n",
        "                    (x_start + (class_idx + 2) * cell_width, y_offset),\n",
        "                    self.config.background_color,\n",
        "                    cv2.FILLED,\n",
        "                )\n",
        "\n",
        "                # Draw count\n",
        "                text_x = (\n",
        "                    x_start\n",
        "                    + (class_idx + 1) * cell_width\n",
        "                    + (cell_width - text_width) // 2\n",
        "                )\n",
        "                cv2.putText(\n",
        "                    frame,\n",
        "                    count_text,\n",
        "                    (text_x, y_offset - 2),\n",
        "                    self.config.text_font,\n",
        "                    self.config.text_scale,\n",
        "                    self.config.text_color,\n",
        "                    self.config.text_thickness,\n",
        "                    self.config.text_line_type,\n",
        "                )\n",
        "\n",
        "            # Move to next row\n",
        "            y_offset += cell_height\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWBS1U1Q4Qpj"
      },
      "source": [
        "##### Counter Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NlK9hxI54Qpj"
      },
      "outputs": [],
      "source": [
        "class CounterManager:\n",
        "    \"\"\"Integrates object counting with the main video processing pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, class_names, allowed_classes, config):\n",
        "        self.config = config\n",
        "        self.counter = ObjectCounter(allowed_classes, class_names)\n",
        "        self.visualizer = CounterVisualizer(self.counter, class_names, config)\n",
        "        self.drawer = LineDrawer(config\n",
        "                        ) if self.config.video_config.enable_drawer else None\n",
        "\n",
        "        self.get_lines_geometry()\n",
        "\n",
        "    def get_lines_geometry(self):\n",
        "        \"\"\"Get current lines geometry\"\"\"\n",
        "        if self.drawer:\n",
        "            self.lines_geometry = self.drawer.run()\n",
        "            # Load lines from file\n",
        "            self.lines_geometry = self.load_lines_geometry()\n",
        "        else:\n",
        "            # Load lines from file\n",
        "            self.lines_geometry = self.load_lines_geometry()\n",
        "\n",
        "\n",
        "    def process_frame(self, frame, tracks):\n",
        "        \"\"\"Process tracks for a frame and draw visualization\"\"\"\n",
        "        # Update counters with current tracks\n",
        "        self.counter.update(tracks, self.lines_geometry)\n",
        "        # Draw visualization on the frame\n",
        "        frame = self.visualizer.draw(frame, self.lines_geometry)\n",
        "        return frame\n",
        "\n",
        "    def load_lines_geometry(self):\n",
        "        \"\"\"\n",
        "        Load lines geometry from a JSON file.\n",
        "        Returns a list of line configurations.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            #with open(\"/content/drive/MyDrive/16_AdvancedTracking/lines_geometry.json\", \"r\") as file:\n",
        "            with open(\"../src/counter/lines_geometry.json\", \"r\") as file:\n",
        "                lines = json.load(file)\n",
        "                # Initialize tracking state for each line\n",
        "                for line in lines:\n",
        "                    line.setdefault(\"counts\", {\"up\": {}, \"down\": {}})\n",
        "                    line.setdefault(\"tracked_objects\", {})\n",
        "                return lines\n",
        "        except FileNotFoundError:\n",
        "            return []\n",
        "\n",
        "    def get_counts(self):\n",
        "        \"\"\"Get current counts for all lines\"\"\"\n",
        "        return self.counter.get_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMbDSZMq4Qpj"
      },
      "source": [
        "##### Counter Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nol5Qvfq4Qpj"
      },
      "outputs": [],
      "source": [
        "class CounterSummary:\n",
        "    def __init__(self, model_handler, config):\n",
        "        self.model_handler = model_handler\n",
        "        self.config = config\n",
        "        self.model_name = config.model_name\n",
        "        self.summary_data = {}\n",
        "        self.output_file = f\"Summary/counter_summary_{self.model_name}.txt\"\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "        self.frame_count = 0\n",
        "        self.fps_measurements = []\n",
        "        self.detection_counts = 0\n",
        "        self.processing_stats = {\n",
        "            \"input_video\": config.video_config.input_path,\n",
        "            \"output_video\": config.video_config.output_path,\n",
        "            \"tracking_algorithm\": config.video_config.tracking_algorithm\n",
        "            if config.video_config.enable_tracking\n",
        "            else \"None\",\n",
        "            \"frame_skip\": config.video_config.frame_skip,\n",
        "        }\n",
        "\n",
        "    def start_processing(self):\n",
        "        \"\"\"Record the start time of processing\"\"\"\n",
        "        self.start_time = datetime.datetime.now()\n",
        "\n",
        "    def end_processing(self):\n",
        "        \"\"\"Record the end time of processing\"\"\"\n",
        "        self.end_time = datetime.datetime.now()\n",
        "\n",
        "    def update_frame_stats(self, fps, detection_count=0):\n",
        "        \"\"\"Update per-frame statistics\"\"\"\n",
        "        self.frame_count += 1\n",
        "        if fps > 0:  # Ignore zero FPS values\n",
        "            self.fps_measurements.append(fps)\n",
        "        self.detection_counts += detection_count\n",
        "\n",
        "    def update_from_lines(self, lines_geometry):\n",
        "        \"\"\"Update summary data from the lines geometry data\"\"\"\n",
        "        if self.model_name not in self.summary_data:\n",
        "            self.summary_data[self.model_name] = {}\n",
        "\n",
        "        for line in lines_geometry:\n",
        "            line_name = line[\"name\"]\n",
        "            if line_name not in self.summary_data[self.model_name]:\n",
        "                self.summary_data[self.model_name][line_name] = {\n",
        "                    \"up\": line[\"counts\"][\"up\"].copy(),\n",
        "                    \"down\": line[\"counts\"][\"down\"].copy(),\n",
        "                    \"total_up\": sum(line[\"counts\"][\"up\"].values()),\n",
        "                    \"total_down\": sum(line[\"counts\"][\"down\"].values()),\n",
        "                }\n",
        "\n",
        "    def get_processing_stats(self):\n",
        "        \"\"\"Calculate processing statistics\"\"\"\n",
        "        stats = self.processing_stats.copy()\n",
        "\n",
        "        # Get model parameters\n",
        "        total_params, trained_params = self.model_handler.param\n",
        "        stats[\"total_parameters\"] = total_params\n",
        "        stats[\"trainable_parameters\"] = trained_params\n",
        "\n",
        "        # Calculate timing information\n",
        "        if self.start_time and self.end_time:\n",
        "            processing_duration = self.end_time - self.start_time\n",
        "            stats[\"processing_start\"] = self.start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            stats[\"processing_end\"] = self.end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            stats[\"processing_duration\"] = str(processing_duration)\n",
        "            stats[\"processing_seconds\"] = processing_duration.total_seconds()\n",
        "\n",
        "        # Calculate FPS statistics\n",
        "        if self.fps_measurements:\n",
        "            stats[\"fps_min\"] = min(self.fps_measurements)\n",
        "            stats[\"fps_max\"] = max(self.fps_measurements)\n",
        "            stats[\"fps_mean\"] = sum(self.fps_measurements) / len(self.fps_measurements)\n",
        "            stats[\"fps_median\"] = sorted(self.fps_measurements)[\n",
        "                len(self.fps_measurements) // 2\n",
        "            ]\n",
        "\n",
        "        # Other statistics\n",
        "        stats[\"frames_processed\"] = self.frame_count\n",
        "        stats[\"detections_total\"] = self.detection_counts\n",
        "        if self.frame_count > 0:\n",
        "            stats[\"detections_per_frame_avg\"] = self.detection_counts / self.frame_count\n",
        "\n",
        "        # Calculate total counts across all lines\n",
        "        total_up = 0\n",
        "        total_down = 0\n",
        "        total_objects = 0\n",
        "\n",
        "        for model, lines in self.summary_data.items():\n",
        "            for line_name, counts in lines.items():\n",
        "                total_up += counts[\"total_up\"]\n",
        "                total_down += counts[\"total_down\"]\n",
        "                total_objects += counts[\"total_up\"] + counts[\"total_down\"]\n",
        "\n",
        "        stats[\"total_up\"] = total_up\n",
        "        stats[\"total_down\"] = total_down\n",
        "        stats[\"total_objects\"] = total_objects\n",
        "\n",
        "        # Device information\n",
        "        if torch.cuda.is_available():\n",
        "            stats[\"device\"] = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
        "            stats[\"cuda_memory_allocated_peak\"] = (\n",
        "                f\"{torch.cuda.max_memory_allocated() / 1e9:.2f} GB\"\n",
        "            )\n",
        "            stats[\"cuda_memory_reserved_peak\"] = (\n",
        "                f\"{torch.cuda.max_memory_reserved() / 1e9:.2f} GB\"\n",
        "            )\n",
        "        else:\n",
        "            stats[\"device\"] = \"CPU\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print the summary to console\"\"\"\n",
        "        stats = self.get_processing_stats()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"OBJECT COUNTING SUMMARY FOR MODEL: {self.model_name}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Print processing statistics\n",
        "        print(\"\\nPROCESSING INFORMATION:\")\n",
        "        print(f\"  Input Video: {stats['input_video']}\")\n",
        "        print(f\"  Output Video: {stats['output_video']}\")\n",
        "        print(f\"  Model: {self.model_name}\")\n",
        "        print(f\"  Tracking Algorithm: {stats['tracking_algorithm']}\")\n",
        "        print(f\"  Device: {stats['device']}\")\n",
        "        if \"cuda_memory_allocated_peak\" in stats:\n",
        "            print(f\"  Peak GPU Memory: {stats['cuda_memory_allocated_peak']}\")\n",
        "\n",
        "        print(\"\\nTIMING:\")\n",
        "        if \"processing_start\" in stats:\n",
        "            print(f\"  Start: {stats['processing_start']}\")\n",
        "            print(f\"  End: {stats['processing_end']}\")\n",
        "            print(f\"  Duration: {stats['processing_duration']}\")\n",
        "            print(f\"  Total Seconds: {stats['processing_seconds']:.2f}\")\n",
        "\n",
        "        print(\"\\nPERFORMANCE:\")\n",
        "        if \"fps_mean\" in stats:\n",
        "            print(f\"  Average FPS: {stats['fps_mean']:.2f}\")\n",
        "            print(f\"  Median FPS: {stats['fps_median']:.2f}\")\n",
        "            print(f\"  Min FPS: {stats['fps_min']:.2f}\")\n",
        "            print(f\"  Max FPS: {stats['fps_max']:.2f}\")\n",
        "        print(f\"  Frames Processed: {stats['frames_processed']}\")\n",
        "        print(f\"  Frame Skip Rate: {stats['frame_skip']}\")\n",
        "        if \"detections_per_frame_avg\" in stats:\n",
        "            print(\n",
        "                f\"  Average Detections per Frame: {stats['detections_per_frame_avg']:.2f}\"\n",
        "            )\n",
        "\n",
        "        print(\"\\nCOUNTING STATISTICS:\")\n",
        "        print(f\"  Total Up: {stats['total_up']}\")\n",
        "        print(f\"  Total Down: {stats['total_down']}\")\n",
        "        print(f\"  Total Objects: {stats['total_objects']}\")\n",
        "\n",
        "        # Print detailed counting data\n",
        "        for model, lines in self.summary_data.items():\n",
        "            for line_name, counts in lines.items():\n",
        "                print(f\"\\nLine: {line_name}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "                # Print class-specific counts by direction\n",
        "                for direction in [\"up\", \"down\"]:\n",
        "                    print(f\"\\n{direction.upper()} Direction:\")\n",
        "                    if counts[direction]:\n",
        "                        for class_id, count in counts[direction].items():\n",
        "                            class_name = (\n",
        "                                self.config.class_names[int(class_id)]\n",
        "                                if int(class_id) < len(self.config.class_names)\n",
        "                                else f\"Class {int(class_id)}\"\n",
        "                            )\n",
        "                            print(f\"  {class_name}: {count}\")\n",
        "                    else:\n",
        "                        print(\"  No objects counted\")\n",
        "\n",
        "                # Print totals\n",
        "                print(f\"\\nTotals:\")\n",
        "                print(f\"  Total Up: {counts['total_up']}\")\n",
        "                print(f\"  Total Down: {counts['total_down']}\")\n",
        "                print(f\"  Overall Total: {counts['total_up'] + counts['total_down']}\")\n",
        "\n",
        "    def export_to_file(self):\n",
        "        \"\"\"Export the summary to a text file\"\"\"\n",
        "        stats = self.get_processing_stats()\n",
        "\n",
        "        with open(self.output_file, \"w\") as f:\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(f\"OBJECT COUNTING SUMMARY FOR MODEL: {self.model_name}\\n\")\n",
        "            f.write(f\"  Total Parameters: {stats['total_parameters']}\\n\")\n",
        "            f.write(f\"  Trainable Parameters: {stats['trainable_parameters']}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            # Write processing statistics\n",
        "            f.write(\"PROCESSING INFORMATION:\\n\")\n",
        "            f.write(f\"  Input Video: {stats['input_video']}\\n\")\n",
        "            f.write(f\"  Output Video: {stats['output_video']}\\n\")\n",
        "            f.write(f\"  Model: {self.model_name}\\n\")\n",
        "            f.write(f\"  Tracking Algorithm: {stats['tracking_algorithm']}\\n\")\n",
        "            f.write(f\"  Device: {stats['device']}\\n\")\n",
        "            if \"cuda_memory_allocated_peak\" in stats:\n",
        "                f.write(f\"  Peak GPU Memory: {stats['cuda_memory_allocated_peak']}\\n\")\n",
        "\n",
        "            f.write(\"\\nTIMING:\\n\")\n",
        "            if \"processing_start\" in stats:\n",
        "                f.write(f\"  Start: {stats['processing_start']}\\n\")\n",
        "                f.write(f\"  End: {stats['processing_end']}\\n\")\n",
        "                f.write(f\"  Duration: {stats['processing_duration']}\\n\")\n",
        "                f.write(f\"  Total Seconds: {stats['processing_seconds']:.2f}\\n\")\n",
        "\n",
        "            f.write(\"\\nPERFORMANCE:\\n\")\n",
        "            if \"fps_mean\" in stats:\n",
        "                f.write(f\"  Average FPS: {stats['fps_mean']:.2f}\\n\")\n",
        "                f.write(f\"  Median FPS: {stats['fps_median']:.2f}\\n\")\n",
        "                f.write(f\"  Min FPS: {stats['fps_min']:.2f}\\n\")\n",
        "                f.write(f\"  Max FPS: {stats['fps_max']:.2f}\\n\")\n",
        "            f.write(f\"  Frames Processed: {stats['frames_processed']}\\n\")\n",
        "            f.write(f\"  Frame Skip Rate: {stats['frame_skip']}\\n\")\n",
        "            if \"detections_per_frame_avg\" in stats:\n",
        "                f.write(\n",
        "                    f\"  Average Detections per Frame: {stats['detections_per_frame_avg']:.2f}\\n\"\n",
        "                )\n",
        "\n",
        "            f.write(\"\\nCOUNTING STATISTICS:\\n\")\n",
        "            f.write(f\"  Total Up: {stats['total_up']}\\n\")\n",
        "            f.write(f\"  Total Down: {stats['total_down']}\\n\")\n",
        "            f.write(f\"  Total Objects: {stats['total_objects']}\\n\")\n",
        "\n",
        "            # Write detailed counting data\n",
        "            for model, lines in self.summary_data.items():\n",
        "                for line_name, counts in lines.items():\n",
        "                    f.write(f\"\\nLine: {line_name}\\n\")\n",
        "                    f.write(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "                    # Write class-specific counts by direction\n",
        "                    for direction in [\"up\", \"down\"]:\n",
        "                        f.write(f\"\\n{direction.upper()} Direction:\\n\")\n",
        "                        if counts[direction]:\n",
        "                            for class_id, count in counts[direction].items():\n",
        "                                class_name = (\n",
        "                                    self.config.class_names[int(class_id)]\n",
        "                                    if int(class_id) < len(self.config.class_names)\n",
        "                                    else f\"Class {int(class_id)}\"\n",
        "                                )\n",
        "                                f.write(f\"  {class_name}: {count}\\n\")\n",
        "                        else:\n",
        "                            f.write(\"  No objects counted\\n\")\n",
        "\n",
        "                    # Write totals\n",
        "                    f.write(f\"\\nTotals:\\n\")\n",
        "                    f.write(f\"  Total Up: {counts['total_up']}\\n\")\n",
        "                    f.write(f\"  Total Down: {counts['total_down']}\\n\")\n",
        "                    f.write(\n",
        "                        f\"  Overall Total: {counts['total_up'] + counts['total_down']}\\n\\n\"\n",
        "                    )\n",
        "\n",
        "        #print(f\"\\nSummary exported to {self.output_file}\")\n",
        "\n",
        "    def export_to_csv(self):\n",
        "        \"\"\"Export summary data to CSV for further analysis\"\"\"\n",
        "        csv_file = f\"Summary/counter_summary_{self.model_name}.csv\"\n",
        "\n",
        "        # Create data for CSV\n",
        "        rows = []\n",
        "\n",
        "        # Add header row\n",
        "        header = [\"Model\", \"Line\", \"Direction\", \"Class_ID\", \"Class_Name\", \"Count\"]\n",
        "        rows.append(header)\n",
        "\n",
        "        # Add data rows\n",
        "        for model, lines in self.summary_data.items():\n",
        "            for line_name, counts in lines.items():\n",
        "                for direction in [\"up\", \"down\"]:\n",
        "                    for class_id, count in counts[direction].items():\n",
        "                        class_name = (\n",
        "                            self.config.class_names[int(class_id)]\n",
        "                            if int(class_id) < len(self.config.class_names)\n",
        "                            else f\"Class {int(class_id)}\"\n",
        "                        )\n",
        "                        row = [model, line_name, direction, class_id, class_name, count]\n",
        "                        rows.append(row)\n",
        "\n",
        "        # Write to CSV\n",
        "        with open(csv_file, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(rows)\n",
        "\n",
        "        #print(f\"CSV data exported to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZ2W1hW4Qpk"
      },
      "source": [
        "#### Display Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zu3LgvRx4Qpk"
      },
      "outputs": [],
      "source": [
        "class DisplayBase:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def _draw_box(self, frame, box):\n",
        "        \"\"\"Draw bounding box on frame\"\"\"\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cv2.rectangle(\n",
        "            frame,\n",
        "            (x1, y1),\n",
        "            (x2, y2),\n",
        "            self.config.box_color,\n",
        "            self.config.box_thickness,\n",
        "            self.config.box_line_type,\n",
        "        )\n",
        "\n",
        "        return x1, y1, x2, y2\n",
        "\n",
        "    def _draw_label(self, frame, text, position):\n",
        "        \"\"\"Draw text label on frame\"\"\"\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            text,\n",
        "            position,\n",
        "            self.config.text_font,\n",
        "            self.config.text_scale,\n",
        "            self.config.text_color,\n",
        "            self.config.text_thickness,\n",
        "            self.config.text_line_type,\n",
        "        )\n",
        "\n",
        "    def _draw_background(self, frame, text):\n",
        "        # Base vertical offset\n",
        "        y_offset = self.config.base_y_offset\n",
        "\n",
        "        # Get text size for background\n",
        "        (text_width, text_height), baseline = cv2.getTextSize(\n",
        "            text,\n",
        "            self.config.text_font,\n",
        "            self.config.text_scale,\n",
        "            self.config.text_thickness\n",
        "        )\n",
        "        # Draw background\n",
        "        cv2.rectangle(\n",
        "            frame,\n",
        "            (10, y_offset - text_height - baseline),\n",
        "            (10 + text_width, y_offset + baseline),\n",
        "            self.config.background_color,\n",
        "            cv2.FILLED,\n",
        "        )\n",
        "\n",
        "    def _draw_fps(self, frame, fps):\n",
        "        \"\"\"Draw FPS counter on frame\"\"\"\n",
        "        if self.config.show_fps:\n",
        "            fps_text = f\"FPS: {fps:.2f}\"\n",
        "            self._draw_background(frame, fps_text)\n",
        "            self._draw_label(frame, fps_text, self.config.fps_position)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hLJ9koTC4Qpk"
      },
      "outputs": [],
      "source": [
        "class DisplayManager(DisplayBase):\n",
        "    def __init__(self, config, class_names):\n",
        "        self.config = config\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def display_frame(self, frame):\n",
        "        self.window_name = self.config.window_name\n",
        "        cv2.namedWindow(self.window_name)\n",
        "        cv2.imshow(self.window_name, frame)\n",
        "        return cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    def draw_detections(self, frame, boxes, scores, labels, fps):\n",
        "        \"\"\"Draw detection boxes and labels\"\"\"\n",
        "        frame_out = frame.copy()\n",
        "\n",
        "        # Draw boxes and labels\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            # Draw box\n",
        "            x1, y1, _, _ = self._draw_box(frame_out, box)\n",
        "            # Draw label\n",
        "            if self.config.show_labels:\n",
        "                label_text = f\"{self.class_names[int(label.item())]} {score:.2f}\"\n",
        "                self._draw_label(\n",
        "                    frame_out, label_text, (x1, y1 - self.config.text_padding)\n",
        "                )\n",
        "        # Draw FPS\n",
        "        self._draw_fps(frame_out, fps)\n",
        "        return frame_out\n",
        "\n",
        "    def draw_tracks(self, frame, tracks, fps):\n",
        "        \"\"\"Draw tracking boxes and labels\"\"\"\n",
        "        frame_out = frame.copy()\n",
        "        # Draw tracks\n",
        "        for track in tracks:\n",
        "            # Get track info\n",
        "            track_id = track.track_id\n",
        "            if hasattr(track, \"to_ltrb\") and callable(track.to_ltrb):\n",
        "                # for DeepSORT\n",
        "                if not track.is_confirmed():\n",
        "                    continue\n",
        "                ltrb = track.to_ltrb()\n",
        "                class_id = track.det_class\n",
        "            elif hasattr(track, \"tlwh\"):\n",
        "                # for ByteTrack (STrack), convert tlwh to ltrb\n",
        "                if not track.is_activated:\n",
        "                    continue\n",
        "                l, t, w, h = track.tlwh  # noqa: E741\n",
        "                ltrb = [l, t, l + w, t + h]\n",
        "                class_id = track.label\n",
        "            else:\n",
        "                raise AttributeError(\n",
        "                    \"Track object does not have 'to_ltrb()' or 'tlwh' attributes.\"\n",
        "                )\n",
        "\n",
        "            # Draw box\n",
        "            x1, y1, _, _ = self._draw_box(frame_out, ltrb)\n",
        "            # Draw label\n",
        "            if self.config.show_labels:\n",
        "                label_text = (\n",
        "                    f\"{self.class_names[int(class_id)]}-{track_id}\"\n",
        "                    if class_id < len(self.class_names)\n",
        "                    else f\"ID-{track_id}\"\n",
        "                )\n",
        "                self._draw_label(\n",
        "                    frame_out, label_text, (x1, y1 - self.config.text_padding)\n",
        "                )\n",
        "        # Draw FPS\n",
        "        self._draw_fps(frame_out, fps)\n",
        "\n",
        "        return frame_out\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up display resources\"\"\"\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPN1ub4o4Qpk"
      },
      "source": [
        "#### Video IO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "D28JzCle4Qpk"
      },
      "outputs": [],
      "source": [
        "class VideoReader:\n",
        "    def __init__(self, video_path):\n",
        "        self.cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    def read_frame(self):\n",
        "        return self.cap.read()\n",
        "\n",
        "    def video_parameters(self):\n",
        "        fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        return fps, width, height, total_frames\n",
        "\n",
        "    def release(self):\n",
        "        self.cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KCWfrHHu4Qpl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VideoWriter:\n",
        "    def __init__(self, output_path, fps, size):\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        self.writer = cv2.VideoWriter(output_path, fourcc, fps, size)\n",
        "\n",
        "    def write(self, frame):\n",
        "        self.writer.write(frame)\n",
        "\n",
        "    def release(self):\n",
        "        self.writer.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqyHX8b84Qpl"
      },
      "source": [
        "#### System / Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uoj5waew4Qpl"
      },
      "outputs": [],
      "source": [
        "# System configuration\n",
        "class SystemConfig:\n",
        "    def __init__(self, model_name, input_path):\n",
        "        self.model_name = model_name\n",
        "        self.input_path = input_path\n",
        "        self.output_path = \"Output/{}_{}.mp4\".format(input_path.split('/')[-1].split('.')[0], self.model_name)\n",
        "        self.model_config = ModelConfig(self.model_name)\n",
        "        self.video_config = VideoProcessorConfig(self.input_path, self.output_path)\n",
        "        self.class_names = AllowedClasses(self.model_name).class_names\n",
        "        self.allowed_classes = AllowedClasses(self.model_name).get_allowed_classes()\n",
        "        self.detector_config = DetectorConfig(self.class_names, self.allowed_classes)\n",
        "        self.tracker_config = {\n",
        "            \"DeepSort\": TrackerConfig_DeepSort(),\n",
        "            \"ByteTrack\": TrackerConfig_ByteTrack(self.video_config.frame_skip),\n",
        "        }\n",
        "        self.drawer_config = DrawerConfig()\n",
        "        self.counter_config = CounterConfig()\n",
        "        self.display_config = DisplayConfig()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Component Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-VnZHcBb4Qpl"
      },
      "outputs": [],
      "source": [
        "class ComponentFactory:\n",
        "    @staticmethod\n",
        "    def create(model_handler, config):\n",
        "        return {\n",
        "            \"detector\": DetectorManager(\n",
        "                model_handler, config.detector_config\n",
        "            ).get_detector(),\n",
        "            \"tracker\": TrackerManager(\n",
        "                config.tracker_config[config.video_config.tracking_algorithm]\n",
        "            )\n",
        "            if config.video_config.enable_tracking\n",
        "            else None,\n",
        "            \"counter\": CounterManager(\n",
        "                class_names=config.detector_config.class_names,\n",
        "                allowed_classes=config.detector_config.allowed_classes,\n",
        "                config=config,\n",
        "            )\n",
        "            if config.video_config.enable_counter\n",
        "            else None,\n",
        "            \"display\": DisplayManager(\n",
        "                config.display_config, config.detector_config.class_names\n",
        "            ),\n",
        "            \"memory\": MemoryManager(cleanup_frequency=100),\n",
        "            \"summary\": CounterSummary(model_handler, config),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOI0z7U4Qpl"
      },
      "source": [
        "#### Video Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TBtpa3ka4Qpl"
      },
      "outputs": [],
      "source": [
        "class VideoProcessor:\n",
        "    def __init__(self, model_handler, config, max_frame):\n",
        "\n",
        "        os.makedirs(\"Output\", exist_ok=True)\n",
        "        os.makedirs(\"Summary\", exist_ok=True)\n",
        "\n",
        "        self.model_handler = model_handler\n",
        "        self.device = model_handler.device\n",
        "        self.config = config\n",
        "        self.max_frame = max_frame\n",
        "\n",
        "        try:\n",
        "            self.model_name = self.model_handler.model.model_name\n",
        "        except AttributeError:\n",
        "            self.model_name = self.model_handler.get_model_name()\n",
        "\n",
        "        # Initialize components\n",
        "        self.components = ComponentFactory.create(model_handler, config)\n",
        "        self.detector   = self.components[\"detector\"]\n",
        "        self.tracker    = self.components[\"tracker\"]\n",
        "        self.counter    = self.components[\"counter\"]\n",
        "        self.display    = self.components[\"display\"]\n",
        "        self.memory     = self.components[\"memory\"]\n",
        "        self.summary    = self.components[\"summary\"]\n",
        "\n",
        "    def process_video(self, video_path, output_path):\n",
        "        video_reader = VideoReader(video_path)\n",
        "        target_size = self.model_handler.image_size\n",
        "        frame_skip = self.config.video_config.frame_skip\n",
        "\n",
        "        # Start summary timing\n",
        "        self.summary.start_processing()\n",
        "\n",
        "        # Get video parameters\n",
        "        fps_orig, width_orig, height_orig, total_frames = video_reader.video_parameters()\n",
        "        fps_adjusted = int(fps_orig / frame_skip)\n",
        "\n",
        "        # Calculate new dimensions\n",
        "        padding_info = ImageProcessor.calculate_dimensions(\n",
        "            width_orig, height_orig, target_size\n",
        "        )\n",
        "\n",
        "        # Initialize video writer if needed\n",
        "        writer = (\n",
        "            VideoWriter(output_path, fps_adjusted, padding_info[\"original_size\"])\n",
        "            if output_path\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        frame_count = 0\n",
        "        with tqdm(total=total_frames) as pbar:\n",
        "            while True and frame_count <= self.max_frame:  # stop process by frame count\n",
        "                ret, frame = video_reader.read_frame()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                if frame_count % frame_skip == 0:\n",
        "                    # Process frame\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    # Detect objects\n",
        "                    if self.detector:\n",
        "                        boxes, scores, labels, resized_image = (\n",
        "                            self.detector.detection_pipeline(frame, padding_info)\n",
        "                        )\n",
        "                        # Update detection count for summary\n",
        "                        self.summary.update_frame_stats(0, len(boxes))\n",
        "\n",
        "                    # Track objects\n",
        "                    if self.tracker:\n",
        "                        tracks = self.tracker.update(frame, boxes, scores, labels)\n",
        "\n",
        "                    # Draw results\n",
        "                    if self.tracker:\n",
        "                        frame_processed = self.display.draw_tracks(\n",
        "                            frame, tracks, 1.0 / (time.time() - start_time)\n",
        "                        )\n",
        "                    else:\n",
        "                        frame_processed = self.display.draw_detections(\n",
        "                            frame,\n",
        "                            boxes,\n",
        "                            scores,\n",
        "                            labels,\n",
        "                            1.0 / (time.time() - start_time),\n",
        "                        )\n",
        "\n",
        "                    # Calculate FPS and update summary\n",
        "                    current_fps = 1.0 / (time.time() - start_time)\n",
        "                    self.summary.update_frame_stats(current_fps)\n",
        "\n",
        "                    # Counter:\n",
        "                    if self.counter and self.tracker:\n",
        "                        frame_processed = self.counter.process_frame(\n",
        "                            frame_processed, tracks\n",
        "                        )\n",
        "\n",
        "                    # Display/save results\n",
        "                    if self.config.video_config.enable_display:\n",
        "                        key = self.display.display_frame(frame_processed)\n",
        "                        if key == ord(\"q\"):  # Quit if 'q' is pressed\n",
        "                            break\n",
        "\n",
        "                    if writer and self.config.video_config.enable_save:\n",
        "                        writer.write(frame_processed)\n",
        "\n",
        "                frame_count += 1\n",
        "\n",
        "                # Call memory cleanup\n",
        "                self.memory.cleanup(frame_count)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        # End summary timing\n",
        "        self.summary.end_processing()\n",
        "\n",
        "        # Generate and export summary\n",
        "        if self.counter:\n",
        "            self.summary.update_from_lines(self.counter.lines_geometry)\n",
        "        #self.summary.print_summary()\n",
        "        self.summary.export_to_file()\n",
        "        self.summary.export_to_csv()\n",
        "\n",
        "        # Cleanup\n",
        "        video_reader.release()\n",
        "        if writer:\n",
        "            writer.release()\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ozDxrKM4Qpm",
        "outputId": "45bad103-2ac5-4945-9fe7-45d97feefe52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 8,956,703\n",
            "Trainable parameters: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 201/5281 [00:11<04:53, 17.31it/s]\n"
          ]
        }
      ],
      "source": [
        "def run():\n",
        "\n",
        "    models_weights = [\n",
        "        \"tf_efficientdet_d1\",\n",
        "        \"tf_efficientdet_d3\",\n",
        "        \"fasterrcnn_resnet50_fpn\",\n",
        "        \"yolox-s.pth\",\n",
        "        \"yolox-m.pth\",\n",
        "        \"yolov5su.pt\",\n",
        "        \"yolov5mu.pt\",\n",
        "        \"yolov8s.pt\",\n",
        "        \"yolov8m.pt\",\n",
        "        \"yolo11s.pt\",\n",
        "        \"yolo11m.pt\",\n",
        "        \"yolo11l.pt\",\n",
        "        \"yolo12s.pt\",\n",
        "        \"yolo12m.pt\",\n",
        "        \"yolo12l.pt\",\n",
        "        ]\n",
        "    \n",
        "    models_weights = [\"yolox-s.pth\"]\n",
        "\n",
        "    for model_name in models_weights:\n",
        "        model_name = model_name.split(\".\")[0]\n",
        "        config = SystemConfig(model_name, \"../data/input/Video1.mp4\")\n",
        "        #config = SystemConfig(model_name, \"/content/drive/MyDrive/16_AdvancedTracking/Video1.mp4\")\n",
        "        model_handler = MODELManager(config.model_config.model_name)\n",
        "        video_processor = VideoProcessor(model_handler, config, max_frame=200)\n",
        "        video_processor.process_video(\n",
        "            config.video_config.input_path, config.video_config.output_path\n",
        "        )\n",
        "#        \n",
        "run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oRLmNu3v4Vij",
        "outputId": "042271e5-25bb-4922-b2ea-4b1ff1ada95c"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# from google.colab import files\n",
        "\n",
        "# folder_path = '/content/Output'\n",
        "# zip_path = '/content/Output.zip'\n",
        "\n",
        "# # Comprimir la carpeta en un archivo ZIP\n",
        "# shutil.make_archive(zip_path.replace('.zip', ''), 'zip', folder_path)\n",
        "\n",
        "# # Descargar el archivo ZIP\n",
        "# files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MNm4mykJ4y4B",
        "outputId": "8a8b247f-ab9f-4399-bf08-959adf6dadce"
      },
      "outputs": [],
      "source": [
        "# folder_path = '/content/Summary'\n",
        "# zip_path = '/content/Summary.zip'\n",
        "\n",
        "# # Comprimir la carpeta en un archivo ZIP\n",
        "# shutil.make_archive(zip_path.replace('.zip', ''), 'zip', folder_path)\n",
        "\n",
        "# # Descargar el archivo ZIP\n",
        "# files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ffmpeg -ss 45 -i Video1_fasterrcnn_resnet50_fpn.mp4 \\\n",
        "#        -ss 45 -i Video1_yolov8s.mp4 \\\n",
        "#        -ss 45 -i Video1_yolo11s.mp4 \\\n",
        "#        -ss 45 -i Video1_yolox-s.mp4 \\\n",
        "#        -ss 45 -i Video1_tf_efficientdet_d3.mp4 \\\n",
        "#        -ss 45 -i Video1_yolox-m.mp4 \\\n",
        "#        -filter_complex \"[0:v]scale=640:360,drawtext=text='Video1_fasterrcnn':x=10:y=10:fontsize=24:fontcolor=white[v0];\\\n",
        "#                         [1:v]scale=640:360,drawtext=text='Video1_yolov8s':x=10:y=10:fontsize=24:fontcolor=white[v1];\\\n",
        "#                         [2:v]scale=640:360,drawtext=text='Video1_yolo11s':x=10:y=10:fontsize=24:fontcolor=white[v2];\\\n",
        "#                         [3:v]scale=640:360,drawtext=text='Video1_yolox-s':x=10:y=10:fontsize=24:fontcolor=white[v3];\\\n",
        "#                         [4:v]scale=640:360,drawtext=text='Video1_tf_efficientdet_d3':x=10:y=10:fontsize=24:fontcolor=white[v4];\\\n",
        "#                         [5:v]scale=640:360,drawtext=text='Video1_yolox-m':x=10:y=10:fontsize=24:fontcolor=white[v5];\\\n",
        "#                         [v0][v1]hstack[top];[v2][v3]hstack[middle];\\\n",
        "#                         [v4][v5]hstack[bottom];[top][middle]vstack[upper];[bottom][upper]vstack[out]\" \\\n",
        "#        -map \"[out]\" -c:v libx264 -crf 23 -preset fast comparative_video_with_names.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ffmpeg -ss 45 -i Video1_fasterrcnn_resnet50_fpn.mp4 \\\n",
        "#        -ss 45 -i Video1_yolo11s.mp4 \\\n",
        "#        -ss 45 -i Video1_tf_efficientdet_d3.mp4 \\\n",
        "#        -ss 45 -i Video1_yolox-m.mp4 \\\n",
        "#        -filter_complex \"[0:v]scale=640:360,drawtext=text='Video1_fasterrcnn':x=W-tw-10:y=10:fontsize=24:fontcolor=white[v0];\\\n",
        "#                         [1:v]scale=640:360,drawtext=text='Video1_yolo11s':x=W-tw-10:y=10:fontsize=24:fontcolor=white[v1];\\\n",
        "#                         [2:v]scale=640:360,drawtext=text='Video1_tf_efficientdet_d3':x=W-tw-10:y=10:fontsize=24:fontcolor=white[v2];\\\n",
        "#                         [3:v]scale=640:360,drawtext=text='Video1_yolox-m':x=W-tw-10:y=10:fontsize=24:fontcolor=white[v3];\\\n",
        "#                         [v0][v1]hstack[top];[v2][v3]hstack[bottom];\\\n",
        "#                         [top][bottom]vstack[out]\" \\\n",
        "#        -map \"[out]\" -c:v libx264 -crf 23 -preset fast comparative_video_4_with_names_right.mp4\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
